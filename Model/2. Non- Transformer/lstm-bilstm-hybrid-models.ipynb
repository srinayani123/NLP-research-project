{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import the required libraries\n# Import the required libraries\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\n# Load the preprocessed data\ndata1=pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=100)\n\n# Undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\n# Calculate class weights\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n#class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n\n\n\n# Build the LSTM model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=10000, output_dim=128))\nmodel.add(LSTM(64, return_sequences=True))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(32))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:27:52.594247Z","iopub.execute_input":"2023-04-22T05:27:52.595334Z","iopub.status.idle":"2023-04-22T05:33:47.021790Z","shell.execute_reply.started":"2023-04-22T05:27:52.595286Z","shell.execute_reply":"2023-04-22T05:33:47.020633Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/20\n2534/2536 [============================>.] - ETA: 0s - loss: 0.3734 - accuracy: 0.8398\nEpoch 2: val_loss improved from 0.39962 to 0.39810, saving model to best_model.h5\n2536/2536 [==============================] - 44s 17ms/step - loss: 0.3734 - accuracy: 0.8397 - val_loss: 0.3981 - val_accuracy: 0.8238\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3279 - accuracy: 0.8628\nEpoch 3: val_loss did not improve from 0.39810\n2536/2536 [==============================] - 42s 16ms/step - loss: 0.3279 - accuracy: 0.8628 - val_loss: 0.4244 - val_accuracy: 0.8188\nEpoch 4/20\n2533/2536 [============================>.] - ETA: 0s - loss: 0.2784 - accuracy: 0.8861\nEpoch 4: val_loss did not improve from 0.39810\n2536/2536 [==============================] - 40s 16ms/step - loss: 0.2785 - accuracy: 0.8861 - val_loss: 0.4604 - val_accuracy: 0.8071\nEpoch 5/20\n2534/2536 [============================>.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9099\nEpoch 5: val_loss did not improve from 0.39810\n2536/2536 [==============================] - 38s 15ms/step - loss: 0.2278 - accuracy: 0.9099 - val_loss: 0.5445 - val_accuracy: 0.7970\nEpoch 5: early stopping\n634/634 [==============================] - 4s 6ms/step - loss: 0.3981 - accuracy: 0.8238\nTest accuracy: 0.8238193988800049\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the pickled file\nwith open('/kaggle/input/models/Lstm_with_tuning_1.pkl', 'rb') as f:\n    l_model = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:57:17.929039Z","iopub.execute_input":"2023-04-23T15:57:17.929531Z","iopub.status.idle":"2023-04-23T15:57:18.973794Z","shell.execute_reply.started":"2023-04-23T15:57:17.929487Z","shell.execute_reply":"2023-04-23T15:57:18.972599Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Keras model archive loading:\nFile Name                                             Modified             Size\nconfig.json                                    2023-04-22 06:17:14         3345\nvariables.h5                                   2023-04-22 06:17:14     15682264\nmetadata.json                                  2023-04-22 06:17:14           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n...layers\n......dense\n.........vars\n............0\n............1\n......dropout\n.........vars\n......dropout_1\n.........vars\n......embedding\n.........vars\n............0\n......lstm\n.........cell\n............vars\n...............0\n...............1\n...............2\n.........vars\n......lstm_1\n.........cell\n............vars\n...............0\n...............1\n...............2\n.........vars\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........17\n.........18\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the pickled file\nwith open('/kaggle/input/models-b/biLstm_without_tuning.pkl', 'rb') as f:\n    b_model = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:21:24.884988Z","iopub.execute_input":"2023-04-23T16:21:24.886109Z","iopub.status.idle":"2023-04-23T16:21:26.511499Z","shell.execute_reply.started":"2023-04-23T16:21:24.886064Z","shell.execute_reply":"2023-04-23T16:21:26.510265Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Keras model archive loading:\nFile Name                                             Modified             Size\nconfig.json                                    2023-04-22 12:00:12         3636\nvariables.h5                                   2023-04-22 12:00:12     16040536\nmetadata.json                                  2023-04-22 12:00:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n...layers\n......bidirectional\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......bidirectional_1\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......dense\n.........vars\n............0\n............1\n......dropout\n.........vars\n......dropout_1\n.........vars\n......embedding\n.........vars\n............0\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........17\n.........18\n.........19\n.........2\n.........20\n.........21\n.........22\n.........23\n.........24\n.........25\n.........26\n.........27\n.........28\n.........29\n.........3\n.........30\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\n","output_type":"stream"}]},{"cell_type":"code","source":"y_predb=b_model.predict(x_test3)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:23:14.590491Z","iopub.execute_input":"2023-04-23T16:23:14.590887Z","iopub.status.idle":"2023-04-23T16:23:55.727848Z","shell.execute_reply.started":"2023-04-23T16:23:14.590851Z","shell.execute_reply":"2023-04-23T16:23:55.726735Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 40s 9ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"testba1=pd.read_csv('/kaggle/input/unbalanced/testu.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:50:18.160514Z","iopub.execute_input":"2023-04-23T16:50:18.161633Z","iopub.status.idle":"2023-04-23T16:50:20.003164Z","shell.execute_reply.started":"2023-04-23T16:50:18.161590Z","shell.execute_reply":"2023-04-23T16:50:20.001942Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"testba1.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:50:20.734937Z","iopub.execute_input":"2023-04-23T16:50:20.735561Z","iopub.status.idle":"2023-04-23T16:50:20.821807Z","shell.execute_reply.started":"2023-04-23T16:50:20.735516Z","shell.execute_reply":"2023-04-23T16:50:20.820751Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# Define the columns for which to make predictions\ncols_to_predict = [col for col in testba1.columns if col not in ['comment_text', 'processed_comment_text', 'comment_text_processed','severe_toxicity','obscene','sexual_explicit','identity_attack','insult','threat','other','gender','religion','race','disability']]\n\n# Add predicted values to the test dataset\nfor i, col in enumerate(cols_to_predict):\n    testba1[col + '_pred'] = y_pred_ba[:, i]\n\n# Export the test dataset with predicted values to a CSV file\ntestba1.to_csv('test_with_predictions_decimals_bilstm_attention_single.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:50:23.181471Z","iopub.execute_input":"2023-04-23T16:50:23.182442Z","iopub.status.idle":"2023-04-23T16:50:25.415845Z","shell.execute_reply.started":"2023-04-23T16:50:23.182387Z","shell.execute_reply":"2023-04-23T16:50:25.414709Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"pred_ba=pd.read_csv('/kaggle/working/test_with_predictions_decimals_bilstm_attention_single.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:50:51.045313Z","iopub.execute_input":"2023-04-23T16:50:51.046037Z","iopub.status.idle":"2023-04-23T16:50:52.580834Z","shell.execute_reply.started":"2023-04-23T16:50:51.046000Z","shell.execute_reply":"2023-04-23T16:50:52.579721Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"pred_ba.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:50:55.124156Z","iopub.execute_input":"2023-04-23T16:50:55.124653Z","iopub.status.idle":"2023-04-23T16:50:55.149202Z","shell.execute_reply.started":"2023-04-23T16:50:55.124608Z","shell.execute_reply":"2023-04-23T16:50:55.148059Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"                                        comment_text  severe_toxicity  \\\n0  So between the 2 civil lawyers going for the j...                0   \n1  Hope they have bullet proof glass and bomb bar...                0   \n2  \"...They realize the inter-connectedness betwe...                0   \n3  I'm a Raider fan, but I agree with Finley.  Th...                0   \n4  I voted for Trump and it was not for any reaso...                0   \n\n   obscene  sexual_explicit  identity_attack  insult  threat  other  gender  \\\n0        0                0                0       0       0      1       0   \n1        0                0                0       0       0      1       0   \n2        0                0                0       0       0      1       0   \n3        0                0                0       0       0      1       0   \n4        0                0                0       0       0      1       0   \n\n   religion  race  disability  Target  \\\n0         0     0           0       0   \n1         0     0           0       0   \n2         0     0           0       0   \n3         0     0           0       0   \n4         0     0           0       0   \n\n                              processed_comment_text  \\\n0  ['civil', 'lawyer', 'going', 'job', 'one', 'st...   \n1  ['hope', 'bullet', 'proof', 'glass', 'bomb', '...   \n2  ['they', 'realize', 'interconnectedness', 'nat...   \n3  ['raider', 'fan', 'agree', 'finley', 'these', ...   \n4  ['voted', 'trump', 'reason', 'article', 'faceb...   \n\n                              comment_text_processed  Target_pred  \n0  civil lawyer going job one stellar reputation ...     0.065515  \n1    hope bullet proof glass bomb barrier well armed     0.488264  \n2  they realize interconnectedness nation world n...     0.043872  \n3  raider fan agree finley these player sit anthe...     0.203256  \n4  voted trump reason article facebook what mains...     0.073170  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>sexual_explicit</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>other</th>\n      <th>gender</th>\n      <th>religion</th>\n      <th>race</th>\n      <th>disability</th>\n      <th>Target</th>\n      <th>processed_comment_text</th>\n      <th>comment_text_processed</th>\n      <th>Target_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>So between the 2 civil lawyers going for the j...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['civil', 'lawyer', 'going', 'job', 'one', 'st...</td>\n      <td>civil lawyer going job one stellar reputation ...</td>\n      <td>0.065515</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hope they have bullet proof glass and bomb bar...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['hope', 'bullet', 'proof', 'glass', 'bomb', '...</td>\n      <td>hope bullet proof glass bomb barrier well armed</td>\n      <td>0.488264</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"...They realize the inter-connectedness betwe...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['they', 'realize', 'interconnectedness', 'nat...</td>\n      <td>they realize interconnectedness nation world n...</td>\n      <td>0.043872</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I'm a Raider fan, but I agree with Finley.  Th...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['raider', 'fan', 'agree', 'finley', 'these', ...</td>\n      <td>raider fan agree finley these player sit anthe...</td>\n      <td>0.203256</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I voted for Trump and it was not for any reaso...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['voted', 'trump', 'reason', 'article', 'faceb...</td>\n      <td>voted trump reason article facebook what mains...</td>\n      <td>0.073170</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test4","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:01:38.248499Z","iopub.execute_input":"2023-04-23T16:01:38.248937Z","iopub.status.idle":"2023-04-23T16:01:38.272670Z","shell.execute_reply.started":"2023-04-23T16:01:38.248897Z","shell.execute_reply":"2023-04-23T16:01:38.271575Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"                                             comment_text  severe_toxicity  \\\n0       So between the 2 civil lawyers going for the j...                0   \n1       Hope they have bullet proof glass and bomb bar...                0   \n2       \"...They realize the inter-connectedness betwe...                0   \n3       I'm a Raider fan, but I agree with Finley.  Th...                0   \n4       I voted for Trump and it was not for any reaso...                0   \n...                                                   ...              ...   \n134051  Of course the flyer was male.  Males are bette...                0   \n134052  you are spot on  - i wouldn't want to be a dec...                0   \n134053  Sorry - but I think the DNC has already establ...                0   \n134054  Hi, Amira...been ages since your last column, ...                0   \n134055  Sure and allowing an illegal vote or encouragi...                0   \n\n        obscene  sexual_explicit  identity_attack  insult  threat  other  \\\n0             0                0                0       0       0      1   \n1             0                0                0       0       0      1   \n2             0                0                0       0       0      1   \n3             0                0                0       0       0      1   \n4             0                0                0       0       0      1   \n...         ...              ...              ...     ...     ...    ...   \n134051        0                0                0       0       0      1   \n134052        0                0                0       0       0      1   \n134053        0                0                0       0       0      1   \n134054        0                0                0       0       0      1   \n134055        0                0                0       0       0      1   \n\n        gender  religion  race  disability  Target  \\\n0            0         0     0           0       0   \n1            0         0     0           0       0   \n2            0         0     0           0       0   \n3            0         0     0           0       0   \n4            0         0     0           0       0   \n...        ...       ...   ...         ...     ...   \n134051       1         0     0           0       0   \n134052       0         0     0           0       0   \n134053       0         0     0           0       0   \n134054       0         0     0           0       0   \n134055       0         0     0           0       0   \n\n                                   processed_comment_text  \\\n0       ['civil', 'lawyer', 'going', 'job', 'one', 'st...   \n1       ['hope', 'bullet', 'proof', 'glass', 'bomb', '...   \n2       ['they', 'realize', 'interconnectedness', 'nat...   \n3       ['raider', 'fan', 'agree', 'finley', 'these', ...   \n4       ['voted', 'trump', 'reason', 'article', 'faceb...   \n...                                                   ...   \n134051  ['course', 'flyer', 'male', 'males', 'better',...   \n134052  ['spot', 'would', 'nt', 'want', 'decider', 'pe...   \n134053  ['sorry', 'think', 'dnc', 'already', 'establis...   \n134054  ['amira', 'age', 'since', 'last', 'column', 'p...   \n134055  ['sure', 'allowing', 'illegal', 'vote', 'encou...   \n\n                                   comment_text_processed  Target_pred  \n0       civil lawyer going job one stellar reputation ...     0.203585  \n1         hope bullet proof glass bomb barrier well armed     0.358292  \n2       they realize interconnectedness nation world n...     0.029618  \n3       raider fan agree finley these player sit anthe...     0.499466  \n4       voted trump reason article facebook what mains...     0.020921  \n...                                                   ...          ...  \n134051  course flyer male males better suicide female ...     0.857142  \n134052  spot would nt want decider people cop would ha...     0.313334  \n134053  sorry think dnc already established position t...     0.818399  \n134054  amira age since last column post truth environ...     0.280629  \n134055  sure allowing illegal vote encouraging illegal...     0.021685  \n\n[133973 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>sexual_explicit</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>other</th>\n      <th>gender</th>\n      <th>religion</th>\n      <th>race</th>\n      <th>disability</th>\n      <th>Target</th>\n      <th>processed_comment_text</th>\n      <th>comment_text_processed</th>\n      <th>Target_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>So between the 2 civil lawyers going for the j...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['civil', 'lawyer', 'going', 'job', 'one', 'st...</td>\n      <td>civil lawyer going job one stellar reputation ...</td>\n      <td>0.203585</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hope they have bullet proof glass and bomb bar...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['hope', 'bullet', 'proof', 'glass', 'bomb', '...</td>\n      <td>hope bullet proof glass bomb barrier well armed</td>\n      <td>0.358292</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"...They realize the inter-connectedness betwe...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['they', 'realize', 'interconnectedness', 'nat...</td>\n      <td>they realize interconnectedness nation world n...</td>\n      <td>0.029618</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I'm a Raider fan, but I agree with Finley.  Th...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['raider', 'fan', 'agree', 'finley', 'these', ...</td>\n      <td>raider fan agree finley these player sit anthe...</td>\n      <td>0.499466</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I voted for Trump and it was not for any reaso...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['voted', 'trump', 'reason', 'article', 'faceb...</td>\n      <td>voted trump reason article facebook what mains...</td>\n      <td>0.020921</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>134051</th>\n      <td>Of course the flyer was male.  Males are bette...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['course', 'flyer', 'male', 'males', 'better',...</td>\n      <td>course flyer male males better suicide female ...</td>\n      <td>0.857142</td>\n    </tr>\n    <tr>\n      <th>134052</th>\n      <td>you are spot on  - i wouldn't want to be a dec...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['spot', 'would', 'nt', 'want', 'decider', 'pe...</td>\n      <td>spot would nt want decider people cop would ha...</td>\n      <td>0.313334</td>\n    </tr>\n    <tr>\n      <th>134053</th>\n      <td>Sorry - but I think the DNC has already establ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['sorry', 'think', 'dnc', 'already', 'establis...</td>\n      <td>sorry think dnc already established position t...</td>\n      <td>0.818399</td>\n    </tr>\n    <tr>\n      <th>134054</th>\n      <td>Hi, Amira...been ages since your last column, ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['amira', 'age', 'since', 'last', 'column', 'p...</td>\n      <td>amira age since last column post truth environ...</td>\n      <td>0.280629</td>\n    </tr>\n    <tr>\n      <th>134055</th>\n      <td>Sure and allowing an illegal vote or encouragi...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['sure', 'allowing', 'illegal', 'vote', 'encou...</td>\n      <td>sure allowing illegal vote encouraging illegal...</td>\n      <td>0.021685</td>\n    </tr>\n  </tbody>\n</table>\n<p>133973 rows × 16 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test1= pd.read_csv('/kaggle/input/unbalanced/testu.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:34:22.180625Z","iopub.execute_input":"2023-04-22T05:34:22.181620Z","iopub.status.idle":"2023-04-22T05:34:24.959383Z","shell.execute_reply.started":"2023-04-22T05:34:22.181579Z","shell.execute_reply":"2023-04-22T05:34:24.958286Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"test1.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:35:06.601774Z","iopub.execute_input":"2023-04-22T05:35:06.602477Z","iopub.status.idle":"2023-04-22T05:35:06.672636Z","shell.execute_reply.started":"2023-04-22T05:35:06.602439Z","shell.execute_reply":"2023-04-22T05:35:06.671607Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"x_test=test1['comment_text_processed']","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:35:10.859586Z","iopub.execute_input":"2023-04-22T05:35:10.860044Z","iopub.status.idle":"2023-04-22T05:35:10.870037Z","shell.execute_reply.started":"2023-04-22T05:35:10.860004Z","shell.execute_reply":"2023-04-22T05:35:10.868970Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"y_test=test1['Target']","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:35:24.216289Z","iopub.execute_input":"2023-04-22T05:35:24.217378Z","iopub.status.idle":"2023-04-22T05:35:24.222864Z","shell.execute_reply.started":"2023-04-22T05:35:24.217308Z","shell.execute_reply":"2023-04-22T05:35:24.221776Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(x_test)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_test = pad_sequences(x_test, maxlen=100)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:36:09.535100Z","iopub.execute_input":"2023-04-22T05:36:09.535474Z","iopub.status.idle":"2023-04-22T05:36:17.578742Z","shell.execute_reply.started":"2023-04-22T05:36:09.535434Z","shell.execute_reply":"2023-04-22T05:36:17.577637Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.evaluate(x_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:36:26.372734Z","iopub.execute_input":"2023-04-22T05:36:26.373135Z","iopub.status.idle":"2023-04-22T05:37:07.445992Z","shell.execute_reply.started":"2023-04-22T05:36:26.373100Z","shell.execute_reply":"2023-04-22T05:37:07.444996Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 23s 5ms/step - loss: 0.7244 - accuracy: 0.6787\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[0.7243576049804688, 0.678726315498352]"},"metadata":{}}]},{"cell_type":"code","source":"# Save your model\nimport pickle\nfilename = 'Lstm_without_tuning.pkl'\npickle.dump(model, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:38:39.218815Z","iopub.execute_input":"2023-04-22T05:38:39.219567Z","iopub.status.idle":"2023-04-22T05:38:39.371529Z","shell.execute_reply.started":"2023-04-22T05:38:39.219528Z","shell.execute_reply":"2023-04-22T05:38:39.370432Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dropout\n.........vars\n......dropout_1\n.........vars\n......embedding\n.........vars\n............0\n......lstm\n.........cell\n............vars\n...............0\n...............1\n...............2\n.........vars\n......lstm_1\n.........cell\n............vars\n...............0\n...............1\n...............2\n.........vars\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........17\n.........18\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2023-04-22 05:38:39         3342\nvariables.h5                                   2023-04-22 05:38:39     16138968\nmetadata.json                                  2023-04-22 05:38:39           64\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\n# Load the preprocessed data\n#data1=pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\n#data1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\n# Calculate class weights\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the LSTM model\nmodel1 = Sequential()\nmodel1.add(Embedding(input_dim=10000, output_dim=128))\nmodel1.add(LSTM(32, return_sequences=True))\nmodel1.add(Dropout(0.5))\n#model.add(LSTM(32, return_sequences=True))\nmodel1.add(Dropout(0.5))\nmodel1.add(LSTM(16))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model1.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel1.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model1.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:41:39.751864Z","iopub.execute_input":"2023-04-23T15:41:39.752296Z","iopub.status.idle":"2023-04-23T15:46:12.053483Z","shell.execute_reply.started":"2023-04-23T15:41:39.752253Z","shell.execute_reply":"2023-04-23T15:46:12.052464Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3836 - accuracy: 0.8406\nEpoch 2: val_loss improved from 0.39984 to 0.39835, saving model to best_model.h5\n2536/2536 [==============================] - 43s 17ms/step - loss: 0.3836 - accuracy: 0.8406 - val_loss: 0.3984 - val_accuracy: 0.8255\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3452 - accuracy: 0.8590\nEpoch 3: val_loss did not improve from 0.39835\n2536/2536 [==============================] - 43s 17ms/step - loss: 0.3452 - accuracy: 0.8590 - val_loss: 0.4142 - val_accuracy: 0.8202\nEpoch 4/20\n2533/2536 [============================>.] - ETA: 0s - loss: 0.3083 - accuracy: 0.8752\nEpoch 4: val_loss did not improve from 0.39835\n2536/2536 [==============================] - 42s 17ms/step - loss: 0.3082 - accuracy: 0.8752 - val_loss: 0.4745 - val_accuracy: 0.8142\nEpoch 5/20\n2533/2536 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.8930\nEpoch 5: val_loss did not improve from 0.39835\n2536/2536 [==============================] - 40s 16ms/step - loss: 0.2709 - accuracy: 0.8930 - val_loss: 0.4825 - val_accuracy: 0.8028\nEpoch 5: early stopping\n634/634 [==============================] - 4s 6ms/step - loss: 0.3984 - accuracy: 0.8255\nTest accuracy: 0.8255447149276733\n","output_type":"stream"}]},{"cell_type":"code","source":"test3= pd.read_csv('/kaggle/input/unbalanced/testu.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:58:14.229197Z","iopub.execute_input":"2023-04-22T05:58:14.229596Z","iopub.status.idle":"2023-04-22T05:58:15.688995Z","shell.execute_reply.started":"2023-04-22T05:58:14.229557Z","shell.execute_reply":"2023-04-22T05:58:15.687913Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test3.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:58:16.406184Z","iopub.execute_input":"2023-04-22T05:58:16.406872Z","iopub.status.idle":"2023-04-22T05:58:16.480432Z","shell.execute_reply.started":"2023-04-22T05:58:16.406833Z","shell.execute_reply":"2023-04-22T05:58:16.479364Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"x_test2=test3['comment_text_processed']","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:58:19.446659Z","iopub.execute_input":"2023-04-22T05:58:19.447183Z","iopub.status.idle":"2023-04-22T05:58:19.452803Z","shell.execute_reply.started":"2023-04-22T05:58:19.447145Z","shell.execute_reply":"2023-04-22T05:58:19.451628Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"y_test2=test3['Target']","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:58:22.286199Z","iopub.execute_input":"2023-04-22T05:58:22.286573Z","iopub.status.idle":"2023-04-22T05:58:22.292072Z","shell.execute_reply.started":"2023-04-22T05:58:22.286540Z","shell.execute_reply":"2023-04-22T05:58:22.290873Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(x_test2)\nx_test2 = tokenizer.texts_to_sequences(x_test2)\nx_test2 = pad_sequences(x_test2, maxlen=100)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T05:58:50.085865Z","iopub.execute_input":"2023-04-22T05:58:50.086962Z","iopub.status.idle":"2023-04-22T05:58:58.297208Z","shell.execute_reply.started":"2023-04-22T05:58:50.086907Z","shell.execute_reply":"2023-04-22T05:58:58.296147Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model1.evaluate(x_test3,y_test3)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:48:22.587577Z","iopub.execute_input":"2023-04-23T15:48:22.591689Z","iopub.status.idle":"2023-04-23T15:49:03.702490Z","shell.execute_reply.started":"2023-04-23T15:48:22.591637Z","shell.execute_reply":"2023-04-23T15:49:03.701425Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 25s 6ms/step - loss: 0.7150 - accuracy: 0.6761\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"[0.7149592041969299, 0.6761287450790405]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save your model\nimport pickle\nfilename = 'Lstm_with_tuning.pkl'\npickle.dump(model, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:40:09.304305Z","iopub.execute_input":"2023-04-23T15:40:09.305112Z","iopub.status.idle":"2023-04-23T15:40:09.445888Z","shell.execute_reply.started":"2023-04-23T15:40:09.305075Z","shell.execute_reply":"2023-04-23T15:40:09.444887Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dropout\n.........vars\n......dropout_1\n.........vars\n......dropout_2\n.........vars\n......embedding\n.........vars\n............0\n......lstm\n.........cell\n............vars\n...............0\n...............1\n...............2\n.........vars\n......lstm_1\n.........cell\n............vars\n...............0\n...............1\n...............2\n.........vars\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........17\n.........18\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2023-04-23 15:40:09         3490\nvariables.h5                                   2023-04-23 15:40:09     15684080\nmetadata.json                                  2023-04-23 15:40:09           64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import the required libraries\n# Import the required libraries\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\n# Load the preprocessed data\n#data1=pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\n#data1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\n# Calculate class weights\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n#class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n\n\n\n# Build the LSTM model\nmodel1 = Sequential()\nmodel1.add(Embedding(input_dim=10000, output_dim=128))\nmodel1.add(LSTM(32, return_sequences=True))\nmodel1.add(Dropout(0.5))\nmodel1.add(LSTM(16))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model1.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel1.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model1.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T06:03:06.800172Z","iopub.execute_input":"2023-04-22T06:03:06.801150Z","iopub.status.idle":"2023-04-22T06:07:03.677690Z","shell.execute_reply.started":"2023-04-22T06:03:06.801110Z","shell.execute_reply":"2023-04-22T06:07:03.676532Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4581 - accuracy: 0.7938\nEpoch 1: val_loss improved from inf to 0.39612, saving model to best_model.h5\n2536/2536 [==============================] - 82s 31ms/step - loss: 0.4581 - accuracy: 0.7938 - val_loss: 0.3961 - val_accuracy: 0.8284\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3754 - accuracy: 0.8413\nEpoch 2: val_loss did not improve from 0.39612\n2536/2536 [==============================] - 44s 17ms/step - loss: 0.3754 - accuracy: 0.8413 - val_loss: 0.4038 - val_accuracy: 0.8233\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3290 - accuracy: 0.8635\nEpoch 3: val_loss did not improve from 0.39612\n2536/2536 [==============================] - 44s 17ms/step - loss: 0.3290 - accuracy: 0.8635 - val_loss: 0.4358 - val_accuracy: 0.8163\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2812 - accuracy: 0.8841\nEpoch 4: val_loss did not improve from 0.39612\n2536/2536 [==============================] - 42s 16ms/step - loss: 0.2812 - accuracy: 0.8841 - val_loss: 0.4886 - val_accuracy: 0.8127\nEpoch 4: early stopping\n634/634 [==============================] - 4s 6ms/step - loss: 0.3961 - accuracy: 0.8284\nTest accuracy: 0.8284038305282593\n","output_type":"stream"}]},{"cell_type":"code","source":"test4= pd.read_csv('/kaggle/input/unbalanced/testu.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-26T01:46:17.328726Z","iopub.execute_input":"2023-04-26T01:46:17.329813Z","iopub.status.idle":"2023-04-26T01:46:20.487987Z","shell.execute_reply.started":"2023-04-26T01:46:17.329754Z","shell.execute_reply":"2023-04-26T01:46:20.486695Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test4.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T01:46:20.490670Z","iopub.execute_input":"2023-04-26T01:46:20.491572Z","iopub.status.idle":"2023-04-26T01:46:20.576882Z","shell.execute_reply.started":"2023-04-26T01:46:20.491523Z","shell.execute_reply":"2023-04-26T01:46:20.575728Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"x_test3=test4['comment_text_processed']","metadata":{"execution":{"iopub.status.busy":"2023-04-26T01:46:23.146924Z","iopub.execute_input":"2023-04-26T01:46:23.147473Z","iopub.status.idle":"2023-04-26T01:46:23.154579Z","shell.execute_reply.started":"2023-04-26T01:46:23.147426Z","shell.execute_reply":"2023-04-26T01:46:23.153286Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"y_test3=test4['Target']","metadata":{"execution":{"iopub.status.busy":"2023-04-26T01:46:25.572701Z","iopub.execute_input":"2023-04-26T01:46:25.573278Z","iopub.status.idle":"2023-04-26T01:46:25.579740Z","shell.execute_reply.started":"2023-04-26T01:46:25.573231Z","shell.execute_reply":"2023-04-26T01:46:25.578555Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(x_test3)\nx_test3 = tokenizer.texts_to_sequences(x_test3)\nx_test3 = pad_sequences(x_test3, maxlen=145)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-26T01:46:28.269802Z","iopub.execute_input":"2023-04-26T01:46:28.270534Z","iopub.status.idle":"2023-04-26T01:46:38.504300Z","shell.execute_reply.started":"2023-04-26T01:46:28.270494Z","shell.execute_reply":"2023-04-26T01:46:38.503030Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model1.evaluate(x_test3,y_test3)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T06:10:07.313365Z","iopub.execute_input":"2023-04-22T06:10:07.313735Z","iopub.status.idle":"2023-04-22T06:10:48.401070Z","shell.execute_reply.started":"2023-04-22T06:10:07.313702Z","shell.execute_reply":"2023-04-22T06:10:48.399956Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 25s 6ms/step - loss: 0.6548 - accuracy: 0.7034\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"[0.6547689437866211, 0.7034103870391846]"},"metadata":{}}]},{"cell_type":"code","source":"#predict\ny_pred=model1.predict(x_test3)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T06:11:41.540175Z","iopub.execute_input":"2023-04-22T06:11:41.541039Z","iopub.status.idle":"2023-04-22T06:12:23.263084Z","shell.execute_reply.started":"2023-04-22T06:11:41.540998Z","shell.execute_reply":"2023-04-22T06:12:23.261814Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 22s 5ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"#predict\nmodel.evaluate(x_test3,y_test3)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T06:16:07.801456Z","iopub.execute_input":"2023-04-22T06:16:07.802171Z","iopub.status.idle":"2023-04-22T06:16:49.537976Z","shell.execute_reply.started":"2023-04-22T06:16:07.802132Z","shell.execute_reply":"2023-04-22T06:16:49.536872Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 27s 6ms/step - loss: 0.6784 - accuracy: 0.6947\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"[0.6783618330955505, 0.6947071552276611]"},"metadata":{}}]},{"cell_type":"code","source":"# Calculate F1 score\nfrom sklearn.metrics import f1_score\nf1 = f1_score(y_test3, y_pred.round(), average='macro')\nprint('F1 score:', f1)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T06:13:35.530114Z","iopub.execute_input":"2023-04-22T06:13:35.530502Z","iopub.status.idle":"2023-04-22T06:13:35.592928Z","shell.execute_reply.started":"2023-04-22T06:13:35.530467Z","shell.execute_reply":"2023-04-22T06:13:35.591685Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"F1 score: 0.5290231841155956\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate F1 score\nfrom sklearn.metrics import f1_score\nf1 = f1_score(y_test3, y_pred1.round(), average='macro')\nprint('F1 score:', f1)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T06:15:20.687860Z","iopub.execute_input":"2023-04-22T06:15:20.688856Z","iopub.status.idle":"2023-04-22T06:15:20.755389Z","shell.execute_reply.started":"2023-04-22T06:15:20.688805Z","shell.execute_reply":"2023-04-22T06:15:20.754064Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"F1 score: 0.5302414300997712\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save your model\nimport pickle\nfilename = 'Lstm_with_tuning_1.pkl'\npickle.dump(model1, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-04-22T06:17:15.500141Z","iopub.execute_input":"2023-04-22T06:17:15.500527Z","iopub.status.idle":"2023-04-22T06:17:15.648316Z","shell.execute_reply.started":"2023-04-22T06:17:15.500491Z","shell.execute_reply":"2023-04-22T06:17:15.647221Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dropout\n.........vars\n......dropout_1\n.........vars\n......embedding\n.........vars\n............0\n......lstm\n.........cell\n............vars\n...............0\n...............1\n...............2\n.........vars\n......lstm_1\n.........cell\n............vars\n...............0\n...............1\n...............2\n.........vars\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........17\n.........18\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2023-04-22 06:17:15         3345\nvariables.h5                                   2023-04-22 06:17:15     15682264\nmetadata.json                                  2023-04-22 06:17:15           64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import the required libraries\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Download the GloVe word embeddings\n#!wget http://nlp.stanford.edu/data/glove.6B.zip\n#!unzip glove.6B.zip\n\n# Parse the GloVe word embeddings file\nembedding_dict = {}\nwith open('glove.6B.100d.txt', 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], dtype='float32')\n        embedding_dict[word] = vector\nf.close()\n\n# Create an embedding matrix for the tokenizer\nnum_words = min(10000, len(tokenizer.word_index) + 1)\nembedding_matrix = np.zeros((num_words, 100))\nfor word, i in tokenizer.word_index.items():\n    if i >= num_words:\n        continue\n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\n# Calculate class weights\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the LSTM model\nmodel3 = Sequential()\nmodel3.add(Embedding(num_words, 100, weights=[embedding_matrix], input_length=145, trainable=False))\nmodel3.add(LSTM(32, return_sequences=True))\nmodel3.add(Dropout(0.5))\nmodel3.add(LSTM(32, return_sequences=True))\nmodel3.add(Dropout(0.5))\nmodel3.add(LSTM(16))\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model3.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel3.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model3.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T07:32:43.709522Z","iopub.execute_input":"2023-04-22T07:32:43.709934Z","iopub.status.idle":"2023-04-22T07:44:38.547348Z","shell.execute_reply.started":"2023-04-22T07:32:43.709868Z","shell.execute_reply":"2023-04-22T07:44:38.546173Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.7553\nEpoch 1: val_loss improved from inf to 0.44366, saving model to best_model.h5\n2536/2536 [==============================] - 63s 22ms/step - loss: 0.5150 - accuracy: 0.7553 - val_loss: 0.4437 - val_accuracy: 0.8014\nEpoch 2/20\n2534/2536 [============================>.] - ETA: 0s - loss: 0.4462 - accuracy: 0.8020\nEpoch 2: val_loss improved from 0.44366 to 0.41172, saving model to best_model.h5\n2536/2536 [==============================] - 56s 22ms/step - loss: 0.4462 - accuracy: 0.8020 - val_loss: 0.4117 - val_accuracy: 0.8142\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4265 - accuracy: 0.8127\nEpoch 3: val_loss improved from 0.41172 to 0.40369, saving model to best_model.h5\n2536/2536 [==============================] - 56s 22ms/step - loss: 0.4265 - accuracy: 0.8127 - val_loss: 0.4037 - val_accuracy: 0.8197\nEpoch 4/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.4166 - accuracy: 0.8198\nEpoch 4: val_loss improved from 0.40369 to 0.39885, saving model to best_model.h5\n2536/2536 [==============================] - 56s 22ms/step - loss: 0.4166 - accuracy: 0.8198 - val_loss: 0.3988 - val_accuracy: 0.8231\nEpoch 5/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4052 - accuracy: 0.8242\nEpoch 5: val_loss improved from 0.39885 to 0.39632, saving model to best_model.h5\n2536/2536 [==============================] - 56s 22ms/step - loss: 0.4052 - accuracy: 0.8242 - val_loss: 0.3963 - val_accuracy: 0.8250\nEpoch 6/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3984 - accuracy: 0.8275\nEpoch 6: val_loss did not improve from 0.39632\n2536/2536 [==============================] - 55s 22ms/step - loss: 0.3984 - accuracy: 0.8275 - val_loss: 0.3990 - val_accuracy: 0.8250\nEpoch 7/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.8309\nEpoch 7: val_loss improved from 0.39632 to 0.39351, saving model to best_model.h5\n2536/2536 [==============================] - 56s 22ms/step - loss: 0.3931 - accuracy: 0.8309 - val_loss: 0.3935 - val_accuracy: 0.8282\nEpoch 8/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3847 - accuracy: 0.8347\nEpoch 8: val_loss improved from 0.39351 to 0.39254, saving model to best_model.h5\n2536/2536 [==============================] - 56s 22ms/step - loss: 0.3847 - accuracy: 0.8347 - val_loss: 0.3925 - val_accuracy: 0.8272\nEpoch 9/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3820 - accuracy: 0.8378\nEpoch 9: val_loss improved from 0.39254 to 0.39119, saving model to best_model.h5\n2536/2536 [==============================] - 55s 22ms/step - loss: 0.3820 - accuracy: 0.8378 - val_loss: 0.3912 - val_accuracy: 0.8279\nEpoch 10/20\n2534/2536 [============================>.] - ETA: 0s - loss: 0.3768 - accuracy: 0.8397\nEpoch 10: val_loss did not improve from 0.39119\n2536/2536 [==============================] - 56s 22ms/step - loss: 0.3768 - accuracy: 0.8397 - val_loss: 0.3962 - val_accuracy: 0.8232\nEpoch 11/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3732 - accuracy: 0.8416\nEpoch 11: val_loss did not improve from 0.39119\n2536/2536 [==============================] - 55s 22ms/step - loss: 0.3733 - accuracy: 0.8416 - val_loss: 0.3947 - val_accuracy: 0.8280\nEpoch 12/20\n2534/2536 [============================>.] - ETA: 0s - loss: 0.3689 - accuracy: 0.8431\nEpoch 12: val_loss did not improve from 0.39119\n2536/2536 [==============================] - 56s 22ms/step - loss: 0.3689 - accuracy: 0.8430 - val_loss: 0.3968 - val_accuracy: 0.8280\nEpoch 12: early stopping\n634/634 [==============================] - 5s 8ms/step - loss: 0.3912 - accuracy: 0.8279\nTest accuracy: 0.8278616070747375\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import the required libraries\n# Load pre-trained GloVe embeddings\n#from gensim.models import KeyedVectors\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom gensim.models import KeyedVectors\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Download pre-trained GloVe embeddings\n#!wget http://nlp.stanford.edu/data/glove.6B.zip\n#!unzip glove.6B.zip -d glove\n\n# Load the preprocessed data\ndata1=pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=300)\n\n\nglove_model = KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=False, no_header=True)\n\n\n# Create an embedding matrix for the tokenizer vocabulary\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\nfor word, i in tokenizer.word_index.items():\n    if word in glove_model:\n        embedding_matrix[i] = glove_model[word]\n\n# Undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\n# Calculate class weights\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the LSTM model with pre-trained GloVe embeddings\nmodel3 = Sequential()\nmodel3.add(Embedding(len(tokenizer.word_index) + 1, 300, weights=[embedding_matrix], input_length=300, trainable=False))\nmodel3.add(LSTM(128, return_sequences=True))\nmodel3.add(Dropout(0.5))\nmodel3.add(LSTM(64))\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model3.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel3.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model3.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T08:03:46.743453Z","iopub.execute_input":"2023-04-22T08:03:46.743905Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4549 - accuracy: 0.7912\nEpoch 1: val_loss improved from inf to 0.40235, saving model to best_model.h5\n2536/2536 [==============================] - 125s 47ms/step - loss: 0.4549 - accuracy: 0.7912 - val_loss: 0.4023 - val_accuracy: 0.8214\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3964 - accuracy: 0.8256\nEpoch 2: val_loss improved from 0.40235 to 0.39319, saving model to best_model.h5\n2536/2536 [==============================] - 120s 47ms/step - loss: 0.3964 - accuracy: 0.8256 - val_loss: 0.3932 - val_accuracy: 0.8235\nEpoch 3/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8364\nEpoch 3: val_loss improved from 0.39319 to 0.39099, saving model to best_model.h5\n2536/2536 [==============================] - 120s 47ms/step - loss: 0.3751 - accuracy: 0.8364 - val_loss: 0.3910 - val_accuracy: 0.8299\nEpoch 4/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3528 - accuracy: 0.8481\nEpoch 4: val_loss improved from 0.39099 to 0.39056, saving model to best_model.h5\n2536/2536 [==============================] - 119s 47ms/step - loss: 0.3528 - accuracy: 0.8481 - val_loss: 0.3906 - val_accuracy: 0.8294\nEpoch 5/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3825 - accuracy: 0.8308\nEpoch 5: val_loss did not improve from 0.39056\n2536/2536 [==============================] - 118s 47ms/step - loss: 0.3825 - accuracy: 0.8307 - val_loss: 0.3947 - val_accuracy: 0.8273\nEpoch 6/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3293 - accuracy: 0.8590\nEpoch 6: val_loss did not improve from 0.39056\n2536/2536 [==============================] - 120s 47ms/step - loss: 0.3293 - accuracy: 0.8590 - val_loss: 0.4068 - val_accuracy: 0.8277\nEpoch 7/20\n   3/2536 [..............................] - ETA: 1:48 - loss: 0.2594 - accuracy: 0.8958","output_type":"stream"}]},{"cell_type":"code","source":"test7= pd.read_csv('/kaggle/input/unbalanced/testu.csv')\ntest7.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T18:10:22.667052Z","iopub.execute_input":"2023-04-22T18:10:22.667858Z","iopub.status.idle":"2023-04-22T18:10:26.327543Z","shell.execute_reply.started":"2023-04-22T18:10:22.667806Z","shell.execute_reply":"2023-04-22T18:10:26.326406Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"x_test6=test7['comment_text_processed']","metadata":{"execution":{"iopub.status.busy":"2023-04-22T18:10:26.333207Z","iopub.execute_input":"2023-04-22T18:10:26.335865Z","iopub.status.idle":"2023-04-22T18:10:26.342739Z","shell.execute_reply.started":"2023-04-22T18:10:26.335823Z","shell.execute_reply":"2023-04-22T18:10:26.341605Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"y_test6=test7['Target']","metadata":{"execution":{"iopub.status.busy":"2023-04-22T18:10:28.841960Z","iopub.execute_input":"2023-04-22T18:10:28.842910Z","iopub.status.idle":"2023-04-22T18:10:28.848318Z","shell.execute_reply.started":"2023-04-22T18:10:28.842853Z","shell.execute_reply":"2023-04-22T18:10:28.846967Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(x_test6)\nx_test6 = tokenizer.texts_to_sequences(x_test6)\nx_test6 = pad_sequences(x_test6, maxlen=145)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T18:10:32.633919Z","iopub.execute_input":"2023-04-22T18:10:32.634619Z","iopub.status.idle":"2023-04-22T18:10:40.814559Z","shell.execute_reply.started":"2023-04-22T18:10:32.634581Z","shell.execute_reply":"2023-04-22T18:10:40.813442Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model.evaluate(x_test6,y_test6)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T13:47:46.497847Z","iopub.execute_input":"2023-04-22T13:47:46.498244Z","iopub.status.idle":"2023-04-22T13:48:07.115685Z","shell.execute_reply.started":"2023-04-22T13:47:46.498208Z","shell.execute_reply":"2023-04-22T13:48:07.114618Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 20s 5ms/step - loss: 0.6751 - accuracy: 0.7115\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[0.6751114130020142, 0.7115090489387512]"},"metadata":{}}]},{"cell_type":"code","source":"# Import the required libraries\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\n# Calculate class weights\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the bidirectional LSTM model\nmodel2 = Sequential()\nmodel2.add(Embedding(input_dim=10000, output_dim=128))\nmodel2.add(Bidirectional(LSTM(256, return_sequences=True)))\nmodel2.add(Dropout(0.5))\nmodel2.add(Bidirectional(LSTM(128,return_sequences=True)))\nmodel2.add(Dropout(0.5))\nmodel2.add(Bidirectional(LSTM(64)))\nmodel2.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model2.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model2.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel2.load_weights('best_model2.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model2.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:10:40.864952Z","iopub.execute_input":"2023-04-22T09:10:40.865337Z","iopub.status.idle":"2023-04-22T09:28:28.204741Z","shell.execute_reply.started":"2023-04-22T09:10:40.865303Z","shell.execute_reply":"2023-04-22T09:28:28.202117Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.7919\nEpoch 1: val_loss improved from inf to 0.40280, saving model to best_model2.h5\n2536/2536 [==============================] - 282s 107ms/step - loss: 0.4508 - accuracy: 0.7919 - val_loss: 0.4028 - val_accuracy: 0.8229\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.8354\nEpoch 2: val_loss did not improve from 0.40280\n2536/2536 [==============================] - 248s 98ms/step - loss: 0.3771 - accuracy: 0.8354 - val_loss: 0.4037 - val_accuracy: 0.8280\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3416 - accuracy: 0.8527\nEpoch 3: val_loss did not improve from 0.40280\n2536/2536 [==============================] - 247s 97ms/step - loss: 0.3416 - accuracy: 0.8527 - val_loss: 0.4154 - val_accuracy: 0.8148\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.8753\nEpoch 4: val_loss did not improve from 0.40280\n2536/2536 [==============================] - 244s 96ms/step - loss: 0.2985 - accuracy: 0.8753 - val_loss: 0.4438 - val_accuracy: 0.8156\nEpoch 4: early stopping\n634/634 [==============================] - 18s 29ms/step - loss: 0.4028 - accuracy: 0.8229\nTest accuracy: 0.8228827714920044\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import the required libraries\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\n#smote = SMOTE(random_state=42)\nrus = RandomUnderSampler(random_state=42)\n#X_resampled, y_resampled = smote.fit_resample(X, y)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the bidirectional LSTM model\nmodel2 = Sequential()\nmodel2.add(Embedding(input_dim=10000, output_dim=128))\nmodel2.add(Bidirectional(LSTM(32, return_sequences=True)))\nmodel2.add(Dropout(0.5))\n#model2.add(Bidirectional(LSTM(16, return_sequences=True)))\n#model2.add(Dropout(0.5))\nmodel2.add(Bidirectional(LSTM(16)))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model2.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model2.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel2.load_weights('best_model2.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model2.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:10:16.023855Z","iopub.execute_input":"2023-04-23T16:10:16.024248Z","iopub.status.idle":"2023-04-23T16:17:36.577872Z","shell.execute_reply.started":"2023-04-23T16:10:16.024212Z","shell.execute_reply":"2023-04-23T16:17:36.576640Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4606 - accuracy: 0.7904\nEpoch 1: val_loss improved from inf to 0.40654, saving model to best_model2.h5\n2536/2536 [==============================] - 116s 43ms/step - loss: 0.4606 - accuracy: 0.7904 - val_loss: 0.4065 - val_accuracy: 0.8206\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.8393\nEpoch 2: val_loss improved from 0.40654 to 0.40521, saving model to best_model2.h5\n2536/2536 [==============================] - 76s 30ms/step - loss: 0.3810 - accuracy: 0.8393 - val_loss: 0.4052 - val_accuracy: 0.8251\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.8578\nEpoch 3: val_loss did not improve from 0.40521\n2536/2536 [==============================] - 73s 29ms/step - loss: 0.3382 - accuracy: 0.8578 - val_loss: 0.4184 - val_accuracy: 0.8229\nEpoch 4/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.8796\nEpoch 4: val_loss did not improve from 0.40521\n2536/2536 [==============================] - 72s 28ms/step - loss: 0.2902 - accuracy: 0.8796 - val_loss: 0.4626 - val_accuracy: 0.8127\nEpoch 5/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.9007\nEpoch 5: val_loss did not improve from 0.40521\n2536/2536 [==============================] - 73s 29ms/step - loss: 0.2443 - accuracy: 0.9007 - val_loss: 0.5477 - val_accuracy: 0.8005\nEpoch 5: early stopping\n634/634 [==============================] - 6s 10ms/step - loss: 0.4052 - accuracy: 0.8251\nTest accuracy: 0.8250517845153809\n","output_type":"stream"}]},{"cell_type":"code","source":"model2.evaluate(x_test3,y_test3)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:19:01.959955Z","iopub.execute_input":"2023-04-23T16:19:01.961000Z","iopub.status.idle":"2023-04-23T16:20:24.057966Z","shell.execute_reply.started":"2023-04-23T16:19:01.960944Z","shell.execute_reply":"2023-04-23T16:20:24.056749Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 43s 10ms/step - loss: 0.7504 - accuracy: 0.6874\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"[0.7503971457481384, 0.6873549222946167]"},"metadata":{}}]},{"cell_type":"code","source":"\n# Save your model\nimport pickle\nfilename = 'biLstm_without_tuning.pkl'\npickle.dump(model2, open(filename, 'wb'))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T12:00:12.609905Z","iopub.execute_input":"2023-04-22T12:00:12.610597Z","iopub.status.idle":"2023-04-22T12:00:12.778051Z","shell.execute_reply.started":"2023-04-22T12:00:12.610556Z","shell.execute_reply":"2023-04-22T12:00:12.777065Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......bidirectional\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......bidirectional_1\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......dense\n.........vars\n............0\n............1\n......dropout\n.........vars\n......dropout_1\n.........vars\n......embedding\n.........vars\n............0\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........17\n.........18\n.........19\n.........2\n.........20\n.........21\n.........22\n.........23\n.........24\n.........25\n.........26\n.........27\n.........28\n.........29\n.........3\n.........30\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2023-04-22 12:00:12         3636\nvariables.h5                                   2023-04-22 12:00:12     16040536\nmetadata.json                                  2023-04-22 12:00:12           64\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the bidirectional LSTM model with attention layer\ninputs = Input(shape=(145,))\nembedding_layer = Embedding(input_dim=10000, output_dim=128)(inputs)\nlstm_layer1 = Bidirectional(LSTM(32, return_sequences=True))(embedding_layer)\ndropout_layer1 = Dropout(0.5)(lstm_layer1)\nlstm_layer2 = Bidirectional(LSTM(16, return_sequences=True))(dropout_layer1)\ndropout_layer2 = Dropout(0.5)(lstm_layer2)\nattention = Dense(1, activation='relu')(dropout_layer2)\nattention = Flatten()(attention)\nattention = Activation('softmax')(attention)\nattention = RepeatVector(32)(attention)\nattention = Permute([2, 1])(attention)\nweighted = Multiply()([dropout_layer2, attention])\noutput_layer = Dense(1, activation='sigmoid')(weighted)\n\nmodel = Model(inputs=inputs, outputs=output_layer)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T13:24:10.324180Z","iopub.execute_input":"2023-04-22T13:24:10.324583Z","iopub.status.idle":"2023-04-22T13:41:14.454348Z","shell.execute_reply.started":"2023-04-22T13:24:10.324544Z","shell.execute_reply":"2023-04-22T13:41:14.453144Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.6361 - accuracy: 0.6970\nEpoch 1: val_loss improved from inf to 0.57035, saving model to best_model.h5\n2536/2536 [==============================] - 129s 48ms/step - loss: 0.6361 - accuracy: 0.6970 - val_loss: 0.5703 - val_accuracy: 0.7821\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.5367 - accuracy: 0.7886\nEpoch 2: val_loss improved from 0.57035 to 0.49731, saving model to best_model.h5\n2536/2536 [==============================] - 86s 34ms/step - loss: 0.5367 - accuracy: 0.7886 - val_loss: 0.4973 - val_accuracy: 0.8065\nEpoch 3/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.4682 - accuracy: 0.8212\nEpoch 3: val_loss improved from 0.49731 to 0.45897, saving model to best_model.h5\n2536/2536 [==============================] - 84s 33ms/step - loss: 0.4682 - accuracy: 0.8212 - val_loss: 0.4590 - val_accuracy: 0.8172\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4326 - accuracy: 0.8310\nEpoch 4: val_loss improved from 0.45897 to 0.45594, saving model to best_model.h5\n2536/2536 [==============================] - 80s 32ms/step - loss: 0.4326 - accuracy: 0.8310 - val_loss: 0.4559 - val_accuracy: 0.8104\nEpoch 5/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.8352\nEpoch 5: val_loss improved from 0.45594 to 0.44733, saving model to best_model.h5\n2536/2536 [==============================] - 83s 33ms/step - loss: 0.4221 - accuracy: 0.8352 - val_loss: 0.4473 - val_accuracy: 0.8016\nEpoch 6/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3977 - accuracy: 0.8446\nEpoch 6: val_loss improved from 0.44733 to 0.43868, saving model to best_model.h5\n2536/2536 [==============================] - 82s 32ms/step - loss: 0.3977 - accuracy: 0.8446 - val_loss: 0.4387 - val_accuracy: 0.8172\nEpoch 7/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3728 - accuracy: 0.8586\nEpoch 7: val_loss did not improve from 0.43868\n2536/2536 [==============================] - 83s 33ms/step - loss: 0.3728 - accuracy: 0.8586 - val_loss: 0.4500 - val_accuracy: 0.8155\nEpoch 8/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.8613\nEpoch 8: val_loss improved from 0.43868 to 0.43540, saving model to best_model.h5\n2536/2536 [==============================] - 82s 32ms/step - loss: 0.3621 - accuracy: 0.8613 - val_loss: 0.4354 - val_accuracy: 0.8140\nEpoch 9/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3450 - accuracy: 0.8710\nEpoch 9: val_loss did not improve from 0.43540\n2536/2536 [==============================] - 82s 33ms/step - loss: 0.3450 - accuracy: 0.8710 - val_loss: 0.4420 - val_accuracy: 0.8140\nEpoch 10/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.8720\nEpoch 10: val_loss did not improve from 0.43540\n2536/2536 [==============================] - 78s 31ms/step - loss: 0.3456 - accuracy: 0.8720 - val_loss: 0.4649 - val_accuracy: 0.8045\nEpoch 11/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3281 - accuracy: 0.8792\nEpoch 11: val_loss did not improve from 0.43540\n2536/2536 [==============================] - 82s 32ms/step - loss: 0.3280 - accuracy: 0.8793 - val_loss: 0.4565 - val_accuracy: 0.8145\nEpoch 11: early stopping\n634/634 [==============================] - 7s 12ms/step - loss: 0.4354 - accuracy: 0.8140\nTest accuracy: 0.8140367269515991\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install keras-self-attention\n","metadata":{"execution":{"iopub.status.busy":"2023-04-26T01:26:43.227197Z","iopub.execute_input":"2023-04-26T01:26:43.227567Z","iopub.status.idle":"2023-04-26T01:27:00.630797Z","shell.execute_reply.started":"2023-04-26T01:26:43.227533Z","shell.execute_reply":"2023-04-26T01:27:00.629409Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting keras-self-attention\n  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from keras-self-attention) (1.21.6)\nBuilding wheels for collected packages: keras-self-attention\n  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18913 sha256=016e1c364377e04e7eebe0483d806b3b6923bbf2958e7143a43d9736207e2605\n  Stored in directory: /root/.cache/pip/wheels/cb/26/00/2d79e29156bddf85b6c2bccecf43fcb024fb935e3d7a933684\nSuccessfully built keras-self-attention\nInstalling collected packages: keras-self-attention\nSuccessfully installed keras-self-attention-0.51.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, Dropout\nfrom keras.layers import Embedding, GlobalAveragePooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the transformer model with self-attention\ninputs = Input(shape=(145,))\nembedding_layer = Embedding(input_dim=10000, output_dim=128)(inputs)\nlstm_layer1 = Dropout(0.5)(embedding_layer)\nlstm_layer2 = SeqSelfAttention(attention_activation='sigmoid')(lstm_layer1)\npooling_layer = GlobalAveragePooling1D()(lstm_layer2)\noutput_layer = Dense(1, activation='sigmoid')(pooling_layer)\n\nmodel = Model(inputs=inputs, outputs=output_layer)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T13:43:56.961342Z","iopub.execute_input":"2023-04-22T13:43:56.961673Z","iopub.status.idle":"2023-04-22T13:47:26.369802Z","shell.execute_reply.started":"2023-04-22T13:43:56.961641Z","shell.execute_reply":"2023-04-22T13:47:26.368731Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2532/2536 [============================>.] - ETA: 0s - loss: 0.5441 - accuracy: 0.7478\nEpoch 1: val_loss improved from inf to 0.46652, saving model to best_model.h5\n2536/2536 [==============================] - 67s 26ms/step - loss: 0.5439 - accuracy: 0.7479 - val_loss: 0.4665 - val_accuracy: 0.7823\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4212 - accuracy: 0.8225\nEpoch 2: val_loss improved from 0.46652 to 0.43226, saving model to best_model.h5\n2536/2536 [==============================] - 32s 13ms/step - loss: 0.4212 - accuracy: 0.8225 - val_loss: 0.4323 - val_accuracy: 0.8172\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.8335\nEpoch 3: val_loss did not improve from 0.43226\n2536/2536 [==============================] - 27s 11ms/step - loss: 0.3910 - accuracy: 0.8335 - val_loss: 0.4375 - val_accuracy: 0.8182\nEpoch 4/20\n2531/2536 [============================>.] - ETA: 0s - loss: 0.3776 - accuracy: 0.8412\nEpoch 4: val_loss did not improve from 0.43226\n2536/2536 [==============================] - 29s 11ms/step - loss: 0.3777 - accuracy: 0.8411 - val_loss: 0.4389 - val_accuracy: 0.8139\nEpoch 5/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3704 - accuracy: 0.8432\nEpoch 5: val_loss did not improve from 0.43226\n2536/2536 [==============================] - 26s 10ms/step - loss: 0.3704 - accuracy: 0.8432 - val_loss: 0.4435 - val_accuracy: 0.8105\nEpoch 5: early stopping\n634/634 [==============================] - 3s 5ms/step - loss: 0.4323 - accuracy: 0.8172\nTest accuracy: 0.8172138333320618\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the bidirectional LSTM model with attention layer\ninputs = Input(shape=(145,))\nembedding_layer = Embedding(input_dim=10000, output_dim=128)(inputs)\nlstm_layer1 = Bidirectional(LSTM(32, return_sequences=True))(embedding_layer)\nattention = SeqSelfAttention(attention_activation='relu')(lstm_layer1)\ndropout_layer1 = Dropout(0.5)(attention)\nlstm_layer2 = Bidirectional(LSTM(16))(dropout_layer1)\ndropout_layer2 = Dropout(0.5)(lstm_layer2)\npooling_layer = GlobalAveragePooling1D()(attention)\noutput_layer = Dense(1, activation='sigmoid')(pooling_layer)\n\nmodel = Model(inputs=inputs, outputs=output_layer)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T14:03:43.064830Z","iopub.execute_input":"2023-04-22T14:03:43.065258Z","iopub.status.idle":"2023-04-22T14:10:39.979072Z","shell.execute_reply.started":"2023-04-22T14:03:43.065223Z","shell.execute_reply":"2023-04-22T14:10:39.977997Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.7999\nEpoch 1: val_loss improved from inf to 0.40114, saving model to best_model.h5\n2536/2536 [==============================] - 107s 40ms/step - loss: 0.4289 - accuracy: 0.7999 - val_loss: 0.4011 - val_accuracy: 0.8198\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3560 - accuracy: 0.8443\nEpoch 2: val_loss improved from 0.40114 to 0.40084, saving model to best_model.h5\n2536/2536 [==============================] - 68s 27ms/step - loss: 0.3560 - accuracy: 0.8443 - val_loss: 0.4008 - val_accuracy: 0.8256\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2992 - accuracy: 0.8722\nEpoch 3: val_loss did not improve from 0.40084\n2536/2536 [==============================] - 61s 24ms/step - loss: 0.2992 - accuracy: 0.8722 - val_loss: 0.4299 - val_accuracy: 0.8176\nEpoch 4/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.2357 - accuracy: 0.9017\nEpoch 4: val_loss did not improve from 0.40084\n2536/2536 [==============================] - 60s 24ms/step - loss: 0.2357 - accuracy: 0.9017 - val_loss: 0.4947 - val_accuracy: 0.8084\nEpoch 5/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9282\nEpoch 5: val_loss did not improve from 0.40084\n2536/2536 [==============================] - 65s 26ms/step - loss: 0.1785 - accuracy: 0.9282 - val_loss: 0.6044 - val_accuracy: 0.7946\nEpoch 5: early stopping\n634/634 [==============================] - 6s 9ms/step - loss: 0.4008 - accuracy: 0.8256\nTest accuracy: 0.8256433010101318\n","output_type":"stream"}]},{"cell_type":"code","source":"model.evaluate(x_test6,y_test6)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T14:10:55.324302Z","iopub.execute_input":"2023-04-22T14:10:55.324697Z","iopub.status.idle":"2023-04-22T14:11:36.416551Z","shell.execute_reply.started":"2023-04-22T14:10:55.324664Z","shell.execute_reply":"2023-04-22T14:11:36.415483Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 39s 9ms/step - loss: 0.6820 - accuracy: 0.7000\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[0.6820387244224548, 0.7000440359115601]"},"metadata":{}}]},{"cell_type":"code","source":"\nimport pickle\nfilename = 'biLstm+attention_without_tuning.pkl'\npickle.dump(model, open(filename, 'wb'))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T14:18:56.043373Z","iopub.execute_input":"2023-04-22T14:18:56.043791Z","iopub.status.idle":"2023-04-22T14:18:56.215321Z","shell.execute_reply.started":"2023-04-22T14:18:56.043759Z","shell.execute_reply":"2023-04-22T14:18:56.214264Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......bidirectional\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......dense\n.........vars\n............0\n............1\n......embedding\n.........vars\n............0\n......global_average_pooling1d\n.........vars\n......input_layer\n.........vars\n......seq_self_attention\n.........vars\n............0\n............1\n............2\n............3\n............4\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........17\n.........18\n.........19\n.........2\n.........20\n.........21\n.........22\n.........23\n.........24\n.........25\n.........26\n.........27\n.........28\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2023-04-22 14:18:56         3755\nvariables.h5                                   2023-04-22 14:18:56     15954856\nmetadata.json                                  2023-04-22 14:18:56           64\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input, BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the bidirectional LSTM model with attention layer\n# Build the bidirectional LSTM model with attention layer\ninputs = Input(shape=(145,))\nembedding_layer = Embedding(input_dim=10000, output_dim=128)(inputs)\nlstm_layer1 = Bidirectional(LSTM(32, return_sequences=True))(embedding_layer)\nattention = SeqSelfAttention(attention_activation='relu')(lstm_layer1)\ndropout_layer1 = Dropout(0.5)(attention)\nlstm_layer2 = Bidirectional(LSTM(16))(dropout_layer1)\ndropout_layer2 = Dropout(0.5)(lstm_layer2)\npooling_layer = GlobalAveragePooling1D()(attention)\nbatch_norm_layer = BatchNormalization()(pooling_layer)\noutput_layer = Dense(1, activation='sigmoid')(batch_norm_layer)\n\nmodel4 = Model(inputs=inputs, outputs=output_layer)\n\n# Compile the model\nmodel4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model4.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel4.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model4.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T14:21:48.781640Z","iopub.execute_input":"2023-04-22T14:21:48.782037Z","iopub.status.idle":"2023-04-22T14:28:50.132379Z","shell.execute_reply.started":"2023-04-22T14:21:48.781996Z","shell.execute_reply":"2023-04-22T14:28:50.131148Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4328 - accuracy: 0.8014\nEpoch 1: val_loss improved from inf to 0.40129, saving model to best_model.h5\n2536/2536 [==============================] - 102s 38ms/step - loss: 0.4328 - accuracy: 0.8014 - val_loss: 0.4013 - val_accuracy: 0.8252\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.8405\nEpoch 2: val_loss improved from 0.40129 to 0.39823, saving model to best_model.h5\n2536/2536 [==============================] - 64s 25ms/step - loss: 0.3648 - accuracy: 0.8405 - val_loss: 0.3982 - val_accuracy: 0.8248\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3057 - accuracy: 0.8711\nEpoch 3: val_loss did not improve from 0.39823\n2536/2536 [==============================] - 67s 26ms/step - loss: 0.3057 - accuracy: 0.8711 - val_loss: 0.4384 - val_accuracy: 0.8132\nEpoch 4/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.2346 - accuracy: 0.9043\nEpoch 4: val_loss did not improve from 0.39823\n2536/2536 [==============================] - 66s 26ms/step - loss: 0.2346 - accuracy: 0.9043 - val_loss: 0.5182 - val_accuracy: 0.8044\nEpoch 5/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9335\nEpoch 5: val_loss did not improve from 0.39823\n2536/2536 [==============================] - 66s 26ms/step - loss: 0.1711 - accuracy: 0.9335 - val_loss: 0.6658 - val_accuracy: 0.7880\nEpoch 5: early stopping\n634/634 [==============================] - 6s 10ms/step - loss: 0.3982 - accuracy: 0.8248\nTest accuracy: 0.8247559666633606\n","output_type":"stream"}]},{"cell_type":"code","source":"model.evaluate(x_test6,y_test6)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T14:42:16.407966Z","iopub.execute_input":"2023-04-22T14:42:16.409102Z","iopub.status.idle":"2023-04-22T14:43:32.689037Z","shell.execute_reply.started":"2023-04-22T14:42:16.409031Z","shell.execute_reply":"2023-04-22T14:43:32.687997Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 76s 18ms/step - loss: 0.6810 - accuracy: 0.7067\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"[0.6810097694396973, 0.7067468762397766]"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input, Reshape\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the bidirectional LSTM model with attention layer\ninputs = Input(shape=(145,))\nembedding_layer = Embedding(input_dim=10000, output_dim=128)(inputs)\nlstm_layer1 = Bidirectional(LSTM(32, return_sequences=True))(embedding_layer)\nattention = SeqSelfAttention(attention_activation='relu')(lstm_layer1)\ndropout_layer1 = Dropout(0.5)(attention)\nlstm_layer2 = Bidirectional(LSTM(16, return_sequences=True))(dropout_layer1)\ndropout_layer2 = Dropout(0.5)(lstm_layer2)\nlstm_layer3 = Bidirectional(LSTM(8))(dropout_layer2)\ndropout_layer3 = Dropout(0.5)(lstm_layer3)\nflatten_layer = Flatten()(dropout_layer3)\nreshaped_layer = Reshape((1, 16))(flatten_layer)\npooling_layer = GlobalAveragePooling1D()(reshaped_layer)\noutput_layer = Dense(1, activation='sigmoid')(pooling_layer)\n\nmodel = Model(inputs=inputs, outputs=output_layer)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:35:14.717243Z","iopub.execute_input":"2023-04-23T16:35:14.717999Z","iopub.status.idle":"2023-04-23T16:45:33.311618Z","shell.execute_reply.started":"2023-04-23T16:35:14.717957Z","shell.execute_reply":"2023-04-23T16:45:33.310265Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.4480 - accuracy: 0.7942\nEpoch 1: val_loss improved from inf to 0.39482, saving model to best_model.h5\n2536/2536 [==============================] - 165s 60ms/step - loss: 0.4481 - accuracy: 0.7942 - val_loss: 0.3948 - val_accuracy: 0.8246\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.8424\nEpoch 2: val_loss did not improve from 0.39482\n2536/2536 [==============================] - 128s 51ms/step - loss: 0.3667 - accuracy: 0.8424 - val_loss: 0.4047 - val_accuracy: 0.8237\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3069 - accuracy: 0.8723\nEpoch 3: val_loss did not improve from 0.39482\n2536/2536 [==============================] - 117s 46ms/step - loss: 0.3069 - accuracy: 0.8723 - val_loss: 0.4390 - val_accuracy: 0.8145\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2386 - accuracy: 0.9026\nEpoch 4: val_loss did not improve from 0.39482\n2536/2536 [==============================] - 126s 50ms/step - loss: 0.2386 - accuracy: 0.9026 - val_loss: 0.5510 - val_accuracy: 0.8056\nEpoch 4: early stopping\n634/634 [==============================] - 12s 18ms/step - loss: 0.3948 - accuracy: 0.8246\nTest accuracy: 0.8246080875396729\n","output_type":"stream"}]},{"cell_type":"code","source":"model.evaluate(x_test3,y_test3)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:46:04.888239Z","iopub.execute_input":"2023-04-23T16:46:04.888888Z","iopub.status.idle":"2023-04-23T16:47:26.931749Z","shell.execute_reply.started":"2023-04-23T16:46:04.888846Z","shell.execute_reply":"2023-04-23T16:47:26.930647Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 75s 18ms/step - loss: 0.6582 - accuracy: 0.7164\n","output_type":"stream"},{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"[0.6582249402999878, 0.7163906097412109]"},"metadata":{}}]},{"cell_type":"code","source":"y_pred_ba=model.predict(x_test3)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T16:48:01.211662Z","iopub.execute_input":"2023-04-23T16:48:01.212160Z","iopub.status.idle":"2023-04-23T16:49:14.084885Z","shell.execute_reply.started":"2023-04-23T16:48:01.212114Z","shell.execute_reply":"2023-04-23T16:49:14.083754Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 71s 16ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nfilename = 'biLstm+attention_with_tuning_1.pkl'\npickle.dump(model, open(filename, 'wb'))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T14:47:04.983320Z","iopub.execute_input":"2023-04-22T14:47:04.984365Z","iopub.status.idle":"2023-04-22T14:47:05.208224Z","shell.execute_reply.started":"2023-04-22T14:47:04.984321Z","shell.execute_reply":"2023-04-22T14:47:05.207107Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......bidirectional\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......bidirectional_1\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......bidirectional_2\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......dense\n.........vars\n............0\n............1\n......dropout\n.........vars\n......dropout_1\n.........vars\n......dropout_2\n.........vars\n......embedding\n.........vars\n............0\n......flatten\n.........vars\n......global_average_pooling1d\n.........vars\n......input_layer\n.........vars\n......reshape\n.........vars\n......seq_self_attention\n.........vars\n............0\n............1\n............2\n............3\n............4\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........17\n.........18\n.........19\n.........2\n.........20\n.........21\n.........22\n.........23\n.........24\n.........25\n.........26\n.........27\n.........28\n.........29\n.........3\n.........30\n.........31\n.........32\n.........33\n.........34\n.........35\n.........36\n.........37\n.........38\n.........39\n.........4\n.........40\n.........41\n.........42\n.........43\n.........44\n.........45\n.........46\n.........47\n.........48\n.........49\n.........5\n.........50\n.........51\n.........52\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2023-04-22 14:47:04         7051\nvariables.h5                                   2023-04-22 14:47:05     16156256\nmetadata.json                                  2023-04-22 14:47:04           64\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input, GlobalAveragePooling1D, BatchNormalization, Reshape\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the bidirectional LSTM model with attention layer\ninputs = Input(shape=(145,))\nembedding_layer = Embedding(input_dim=10000, output_dim=128)(inputs)\nlstm_layer1 = Bidirectional(LSTM(32, return_sequences=True))(embedding_layer)\nattention = SeqSelfAttention(attention_activation='relu')(lstm_layer1)\ndropout_layer1 = Dropout(0.5)(attention)\nlstm_layer2 = Bidirectional(LSTM(16))(dropout_layer1)\ndropout_layer2 = Dropout(0.5)(lstm_layer2)\nreshape_layer = Reshape((1, 32))(dropout_layer2)\npooling_layer = GlobalAveragePooling1D()(reshape_layer)\nbatch_norm_layer = BatchNormalization()(pooling_layer)\noutput_layer = Dense(1, activation='sigmoid')(batch_norm_layer)\n\nmodel6 = Model(inputs=inputs, outputs=output_layer)\n\n# Compile the model\nmodel6.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model6.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict,callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel6.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss,accuracy = model6.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T15:18:41.807337Z","iopub.execute_input":"2023-04-22T15:18:41.807751Z","iopub.status.idle":"2023-04-22T15:26:49.854708Z","shell.execute_reply.started":"2023-04-22T15:18:41.807715Z","shell.execute_reply":"2023-04-22T15:26:49.853376Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.7910\nEpoch 1: val_loss improved from inf to 0.40246, saving model to best_model.h5\n2536/2536 [==============================] - 134s 49ms/step - loss: 0.4513 - accuracy: 0.7910 - val_loss: 0.4025 - val_accuracy: 0.8216\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.8401\nEpoch 2: val_loss did not improve from 0.40246\n2536/2536 [==============================] - 93s 37ms/step - loss: 0.3742 - accuracy: 0.8401 - val_loss: 0.4148 - val_accuracy: 0.8178\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3170 - accuracy: 0.8705\nEpoch 3: val_loss did not improve from 0.40246\n2536/2536 [==============================] - 94s 37ms/step - loss: 0.3170 - accuracy: 0.8705 - val_loss: 0.4455 - val_accuracy: 0.8147\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.9024\nEpoch 4: val_loss did not improve from 0.40246\n2536/2536 [==============================] - 93s 37ms/step - loss: 0.2505 - accuracy: 0.9024 - val_loss: 0.5401 - val_accuracy: 0.8007\nEpoch 4: early stopping\n634/634 [==============================] - 8s 13ms/step - loss: 0.4025 - accuracy: 0.8216\nTest accuracy: 0.8216010928153992\n","output_type":"stream"}]},{"cell_type":"code","source":"model6.evaluate(x_test6,y_test6)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T15:27:36.884533Z","iopub.execute_input":"2023-04-22T15:27:36.884915Z","iopub.status.idle":"2023-04-22T15:28:58.935208Z","shell.execute_reply.started":"2023-04-22T15:27:36.884881Z","shell.execute_reply":"2023-04-22T15:28:58.934082Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 57s 14ms/step - loss: 0.7687 - accuracy: 0.6311\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"[0.7686564922332764, 0.6310749053955078]"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input, Reshape, GlobalAveragePooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the bidirectional LSTM model with attention layer\ninputs = Input(shape=(145,))\nembedding_layer = Embedding(input_dim=10000, output_dim=128)(inputs)\nlstm_layer1 = Bidirectional(LSTM(32, return_sequences=True))(embedding_layer)\nattention = SeqSelfAttention(attention_activation='relu')(lstm_layer1)\ndropout_layer1 = Dropout(0.5)(attention)\nlstm_layer2 = Bidirectional(LSTM(16, return_sequences=True))(dropout_layer1)\ndropout_layer2 = Dropout(0.5)(lstm_layer2)\nlstm_layer3 = Bidirectional(LSTM(8, return_sequences=True))(dropout_layer2)\ndropout_layer3 = Dropout(0.5)(lstm_layer3)\npooling_layer = GlobalAveragePooling1D()(dropout_layer3)\noutput_layer = Dense(1, activation='sigmoid')(pooling_layer)\n\nmodel = Model(inputs=inputs, outputs=output_layer)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T14:58:49.076298Z","iopub.execute_input":"2023-04-22T14:58:49.076695Z","iopub.status.idle":"2023-04-22T15:09:07.308259Z","shell.execute_reply.started":"2023-04-22T14:58:49.076662Z","shell.execute_reply":"2023-04-22T15:09:07.306967Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4326 - accuracy: 0.7978\nEpoch 1: val_loss improved from inf to 0.39595, saving model to best_model.h5\n2536/2536 [==============================] - 178s 65ms/step - loss: 0.4326 - accuracy: 0.7978 - val_loss: 0.3960 - val_accuracy: 0.8232\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3544 - accuracy: 0.8453\nEpoch 2: val_loss did not improve from 0.39595\n2536/2536 [==============================] - 130s 51ms/step - loss: 0.3544 - accuracy: 0.8453 - val_loss: 0.4084 - val_accuracy: 0.8215\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.8779\nEpoch 3: val_loss did not improve from 0.39595\n2536/2536 [==============================] - 128s 51ms/step - loss: 0.2877 - accuracy: 0.8779 - val_loss: 0.4411 - val_accuracy: 0.8134\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9119\nEpoch 4: val_loss did not improve from 0.39595\n2536/2536 [==============================] - 119s 47ms/step - loss: 0.2155 - accuracy: 0.9119 - val_loss: 0.5572 - val_accuracy: 0.8039\nEpoch 4: early stopping\n634/634 [==============================] - 12s 18ms/step - loss: 0.3960 - accuracy: 0.8232\nTest accuracy: 0.8232278227806091\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input, Reshape, GlobalAveragePooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the bidirectional LSTM model with attention layer\ninputs = Input(shape=(145,))\nembedding_layer = Embedding(input_dim=10000, output_dim=128)(inputs)\nlstm_layer1 = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\nattention = SeqSelfAttention(attention_activation='relu')(lstm_layer1)\ndropout_layer1 = Dropout(0.5)(attention)\nlstm_layer2 = Bidirectional(LSTM(16, return_sequences=True))(dropout_layer1)\ndropout_layer2 = Dropout(0.5)(lstm_layer2)\nlstm_layer3 = Bidirectional(LSTM(8, return_sequences=True))(dropout_layer2)\ndropout_layer3 = Dropout(0.5)(lstm_layer3)\npooling_layer = GlobalAveragePooling1D()(dropout_layer3)\noutput_layer = Dense(1, activation='sigmoid')(pooling_layer)\n\nmodel = Model(inputs=inputs, outputs=output_layer)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T18:13:37.185186Z","iopub.execute_input":"2023-04-22T18:13:37.186242Z","iopub.status.idle":"2023-04-22T18:24:00.493482Z","shell.execute_reply.started":"2023-04-22T18:13:37.186195Z","shell.execute_reply":"2023-04-22T18:24:00.492457Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4367 - accuracy: 0.7953\nEpoch 1: val_loss improved from inf to 0.39317, saving model to best_model.h5\n2536/2536 [==============================] - 183s 68ms/step - loss: 0.4367 - accuracy: 0.7953 - val_loss: 0.3932 - val_accuracy: 0.8266\nEpoch 2/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3586 - accuracy: 0.8437\nEpoch 2: val_loss did not improve from 0.39317\n2536/2536 [==============================] - 129s 51ms/step - loss: 0.3586 - accuracy: 0.8437 - val_loss: 0.3993 - val_accuracy: 0.8233\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2968 - accuracy: 0.8745\nEpoch 3: val_loss did not improve from 0.39317\n2536/2536 [==============================] - 138s 54ms/step - loss: 0.2968 - accuracy: 0.8745 - val_loss: 0.4538 - val_accuracy: 0.8173\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2272 - accuracy: 0.9057\nEpoch 4: val_loss did not improve from 0.39317\n2536/2536 [==============================] - 128s 50ms/step - loss: 0.2272 - accuracy: 0.9057 - val_loss: 0.5540 - val_accuracy: 0.7978\nEpoch 4: early stopping\n634/634 [==============================] - 12s 19ms/step - loss: 0.3932 - accuracy: 0.8266\nTest accuracy: 0.8266292214393616\n","output_type":"stream"}]},{"cell_type":"code","source":"model.evaluate(x_test6,y_test6)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T18:24:36.666062Z","iopub.execute_input":"2023-04-22T18:24:36.666773Z","iopub.status.idle":"2023-04-22T18:25:58.706238Z","shell.execute_reply.started":"2023-04-22T18:24:36.666736Z","shell.execute_reply":"2023-04-22T18:25:58.705126Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 80s 19ms/step - loss: 0.6495 - accuracy: 0.6946\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[0.6494514346122742, 0.6945503950119019]"},"metadata":{}}]},{"cell_type":"code","source":"import pickle\nfilename = 'biLstm+attention_with_tuning_2.pkl'\npickle.dump(model, open(filename, 'wb'))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T15:11:49.362950Z","iopub.execute_input":"2023-04-22T15:11:49.363691Z","iopub.status.idle":"2023-04-22T15:11:49.586210Z","shell.execute_reply.started":"2023-04-22T15:11:49.363652Z","shell.execute_reply":"2023-04-22T15:11:49.585049Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......bidirectional\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......bidirectional_1\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......bidirectional_2\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......dense\n.........vars\n............0\n............1\n......dropout\n.........vars\n......dropout_1\n.........vars\n......dropout_2\n.........vars\n......embedding\n.........vars\n............0\n......global_average_pooling1d\n.........vars\n......input_layer\n.........vars\n......seq_self_attention\n.........vars\n............0\n............1\n............2\n............3\n............4\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........17\n.........18\n.........19\n.........2\n.........20\n.........21\n.........22\n.........23\n.........24\n.........25\n.........26\n.........27\n.........28\n.........29\n.........3\n.........30\n.........31\n.........32\n.........33\n.........34\n.........35\n.........36\n.........37\n.........38\n.........39\n.........4\n.........40\n.........41\n.........42\n.........43\n.........44\n.........45\n.........46\n.........47\n.........48\n.........49\n.........5\n.........50\n.........51\n.........52\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2023-04-22 15:11:49         6670\nvariables.h5                                   2023-04-22 15:11:49     16152784\nmetadata.json                                  2023-04-22 15:11:49           64\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input, Reshape, GlobalAveragePooling1D, BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the bidirectional LSTM model with attention layer\ninputs = Input(shape=(145,))\nembedding_layer = Embedding(input_dim=10000, output_dim=128)(inputs)\nlstm_layer1 = Bidirectional(LSTM(32, return_sequences=True))(embedding_layer)\nbn_layer1 = BatchNormalization()(lstm_layer1)\nattention = SeqSelfAttention(attention_activation='relu')(bn_layer1)\ndropout_layer1 = Dropout(0.5)(attention)\nlstm_layer2 = Bidirectional(LSTM(16, return_sequences=True))(dropout_layer1)\nbn_layer2 = BatchNormalization()(lstm_layer2)\ndropout_layer2 = Dropout(0.5)(bn_layer2)\nlstm_layer3 = Bidirectional(LSTM(8, return_sequences=True))(dropout_layer2)\nbn_layer3 = BatchNormalization()(lstm_layer3)\ndropout_layer3 = Dropout(0.5)(bn_layer3)\npooling_layer = GlobalAveragePooling1D()(dropout_layer3)\noutput_layer = Dense(1, activation='sigmoid')(pooling_layer)\n\nmodel = Model(inputs=inputs, outputs=output_layer)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy= model.evaluate(X_test, y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T15:29:42.437572Z","iopub.execute_input":"2023-04-22T15:29:42.438338Z","iopub.status.idle":"2023-04-22T15:40:01.616373Z","shell.execute_reply.started":"2023-04-22T15:29:42.438298Z","shell.execute_reply":"2023-04-22T15:40:01.615149Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4393 - accuracy: 0.7983\nEpoch 1: val_loss improved from inf to 0.40993, saving model to best_model.h5\n2536/2536 [==============================] - 173s 63ms/step - loss: 0.4393 - accuracy: 0.7983 - val_loss: 0.4099 - val_accuracy: 0.8237\nEpoch 2/20\n2535/2536 [============================>.] - ETA: 0s - loss: 0.3724 - accuracy: 0.8375\nEpoch 2: val_loss did not improve from 0.40993\n2536/2536 [==============================] - 125s 49ms/step - loss: 0.3723 - accuracy: 0.8375 - val_loss: 0.4361 - val_accuracy: 0.8187\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3141 - accuracy: 0.8655\nEpoch 3: val_loss did not improve from 0.40993\n2536/2536 [==============================] - 132s 52ms/step - loss: 0.3141 - accuracy: 0.8655 - val_loss: 0.4909 - val_accuracy: 0.7999\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.8983\nEpoch 4: val_loss did not improve from 0.40993\n2536/2536 [==============================] - 123s 49ms/step - loss: 0.2466 - accuracy: 0.8983 - val_loss: 0.5959 - val_accuracy: 0.8065\nEpoch 4: early stopping\n634/634 [==============================] - 12s 19ms/step - loss: 0.4099 - accuracy: 0.8237\nTest accuracy: 0.8237208127975464\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input, Reshape, GlobalAveragePooling1D, concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the Siamese BiLSTM model with attention layer\nembedding_layer = Embedding(input_dim=10000, output_dim=128)\nlstm_layer1 = Bidirectional(LSTM(32, return_sequences=True))\nattention = SeqSelfAttention(attention_activation='relu')\ndropout_layer1 = Dropout(0.5)\nlstm_layer2 = Bidirectional(LSTM(16, return_sequences=True))\ndropout_layer2 = Dropout(0.5)\nlstm_layer3 = Bidirectional(LSTM(8, return_sequences=True))\ndropout_layer3 = Dropout(0.5)\npooling_layer = GlobalAveragePooling1D()\n\ninput1 = Input(shape=(145,))\ninput2 = Input(shape=(145,))\n\nencoded1 = dropout_layer1(attention(lstm_layer1(embedding_layer(input1))))\nencoded2 = dropout_layer1(attention(lstm_layer1(embedding_layer(input2))))\n\nmerged_layer = concatenate([encoded1, encoded2], axis=-1)\n\nmerged_layer = lstm_layer2(merged_layer)\nmerged_layer = dropout_layer2(merged_layer)\n\nmerged_layer = lstm_layer3(merged_layer)\nmerged_layer = dropout_layer3(merged_layer)\n\nmerged_layer = pooling_layer(merged_layer)\n\noutput_layer = Dense(1, activation='sigmoid')(merged_layer)\n\nmodel = Model(inputs=[input1, input2], outputs=output_layer)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model.fit([X_train, X_train], y_train, batch_size=32, epochs=20, validation_data=([X_test,X_test], y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy= model.evaluate([X_test,X_test], y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T15:43:11.104706Z","iopub.execute_input":"2023-04-22T15:43:11.105203Z","iopub.status.idle":"2023-04-22T15:56:20.317165Z","shell.execute_reply.started":"2023-04-22T15:43:11.105163Z","shell.execute_reply":"2023-04-22T15:56:20.315852Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4325 - accuracy: 0.7987\nEpoch 1: val_loss improved from inf to 0.39798, saving model to best_model.h5\n2536/2536 [==============================] - 218s 80ms/step - loss: 0.4325 - accuracy: 0.7987 - val_loss: 0.3980 - val_accuracy: 0.8279\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3557 - accuracy: 0.8445\nEpoch 2: val_loss did not improve from 0.39798\n2536/2536 [==============================] - 178s 70ms/step - loss: 0.3557 - accuracy: 0.8445 - val_loss: 0.4016 - val_accuracy: 0.8274\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2917 - accuracy: 0.8761\nEpoch 3: val_loss did not improve from 0.39798\n2536/2536 [==============================] - 171s 67ms/step - loss: 0.2917 - accuracy: 0.8761 - val_loss: 0.4652 - val_accuracy: 0.8164\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9099\nEpoch 4: val_loss did not improve from 0.39798\n2536/2536 [==============================] - 177s 70ms/step - loss: 0.2197 - accuracy: 0.9099 - val_loss: 0.5553 - val_accuracy: 0.8030\nEpoch 4: early stopping\n634/634 [==============================] - 16s 25ms/step - loss: 0.3980 - accuracy: 0.8279\nTest accuracy: 0.8279109001159668\n","output_type":"stream"}]},{"cell_type":"code","source":"model.evaluate([x_test6,x_test6],y_test6)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T15:56:55.652711Z","iopub.execute_input":"2023-04-22T15:56:55.653747Z","iopub.status.idle":"2023-04-22T15:59:17.772237Z","shell.execute_reply.started":"2023-04-22T15:56:55.653707Z","shell.execute_reply":"2023-04-22T15:59:17.771008Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 103s 25ms/step - loss: 0.6487 - accuracy: 0.7175\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"[0.6487360000610352, 0.7174803614616394]"},"metadata":{}}]},{"cell_type":"code","source":"import pickle\nfilename = 'siamese_biLstm+attention_without_tuning.pkl'\npickle.dump(model, open(filename, 'wb'))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:00:31.474085Z","iopub.execute_input":"2023-04-22T16:00:31.475347Z","iopub.status.idle":"2023-04-22T16:00:31.720132Z","shell.execute_reply.started":"2023-04-22T16:00:31.475293Z","shell.execute_reply":"2023-04-22T16:00:31.718990Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......bidirectional\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......bidirectional_1\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......bidirectional_2\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......concatenate\n.........vars\n......dense\n.........vars\n............0\n............1\n......dropout\n.........vars\n......dropout_1\n.........vars\n......dropout_2\n.........vars\n......embedding\n.........vars\n............0\n......global_average_pooling1d\n.........vars\n......input_layer\n.........vars\n......input_layer_1\n.........vars\n......seq_self_attention\n.........vars\n............0\n............1\n............2\n............3\n............4\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........17\n.........18\n.........19\n.........2\n.........20\n.........21\n.........22\n.........23\n.........24\n.........25\n.........26\n.........27\n.........28\n.........29\n.........3\n.........30\n.........31\n.........32\n.........33\n.........34\n.........35\n.........36\n.........37\n.........38\n.........39\n.........4\n.........40\n.........41\n.........42\n.........43\n.........44\n.........45\n.........46\n.........47\n.........48\n.........49\n.........5\n.........50\n.........51\n.........52\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2023-04-22 16:00:31         7243\nvariables.h5                                   2023-04-22 16:00:31     16254560\nmetadata.json                                  2023-04-22 16:00:31           64\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input, Reshape, GlobalAveragePooling1D, concatenate, BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the Siamese BiLSTM model with attention layer and BatchNormalization\nembedding_layer = Embedding(input_dim=10000, output_dim=128)\nlstm_layer1 = Bidirectional(LSTM(64, return_sequences=True))\nbn_layer1 = BatchNormalization()\nattention = SeqSelfAttention(attention_activation='relu')\ndropout_layer1 = Dropout(0.5)\nlstm_layer2 = Bidirectional(LSTM(32, return_sequences=True))\nbn_layer2 = BatchNormalization()\ndropout_layer2 = Dropout(0.5)\nlstm_layer3 = Bidirectional(LSTM(16, return_sequences=True))\nbn_layer3 = BatchNormalization()\ndropout_layer3 = Dropout(0.5)\npooling_layer = GlobalAveragePooling1D()\n\ninput1 = Input(shape=(145,))\ninput2 = Input(shape=(145,))\n\nencoded1 = dropout_layer1(attention(bn_layer1(lstm_layer1(embedding_layer(input1)))))\nencoded2 = dropout_layer1(attention(bn_layer1(lstm_layer1(embedding_layer(input2)))))\n\nmerged_layer = concatenate([encoded1, encoded2], axis=-1)\n\nmerged_layer = dropout_layer2(bn_layer2(lstm_layer2(merged_layer)))\n\nmerged_layer = dropout_layer3(bn_layer3(lstm_layer3(merged_layer)))\n\nmerged_layer = pooling_layer(merged_layer)\n\noutput_layer = Dense(1, activation='sigmoid')(merged_layer)\n\nmodel = Model(inputs=[input1, input2], outputs=output_layer)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model.fit([X_train, X_train], y_train, batch_size=32, epochs=20, validation_data=([X_test,X_test], y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy= model.evaluate([X_test,X_test], y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:04:06.704703Z","iopub.execute_input":"2023-04-22T16:04:06.705134Z","iopub.status.idle":"2023-04-22T16:19:29.858231Z","shell.execute_reply.started":"2023-04-22T16:04:06.705099Z","shell.execute_reply":"2023-04-22T16:19:29.857013Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.7929\nEpoch 1: val_loss improved from inf to 0.39313, saving model to best_model.h5\n2536/2536 [==============================] - 253s 93ms/step - loss: 0.4440 - accuracy: 0.7929 - val_loss: 0.3931 - val_accuracy: 0.8269\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3766 - accuracy: 0.8348\nEpoch 2: val_loss did not improve from 0.39313\n2536/2536 [==============================] - 203s 80ms/step - loss: 0.3766 - accuracy: 0.8348 - val_loss: 0.4190 - val_accuracy: 0.8238\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3307 - accuracy: 0.8586\nEpoch 3: val_loss did not improve from 0.39313\n2536/2536 [==============================] - 202s 80ms/step - loss: 0.3307 - accuracy: 0.8586 - val_loss: 0.4346 - val_accuracy: 0.8130\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2726 - accuracy: 0.8846\nEpoch 4: val_loss did not improve from 0.39313\n2536/2536 [==============================] - 205s 81ms/step - loss: 0.2726 - accuracy: 0.8846 - val_loss: 0.4589 - val_accuracy: 0.8074\nEpoch 4: early stopping\n634/634 [==============================] - 18s 28ms/step - loss: 0.3931 - accuracy: 0.8269\nTest accuracy: 0.8269249796867371\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import *\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input, Reshape, GlobalAveragePooling1D, concatenate, BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the Siamese BiLSTM model with attention layer\nembedding_layer = Embedding(input_dim=10000, output_dim=128)\nbatch_norm_layer = BatchNormalization()\nlstm_layer1 = Bidirectional(LSTM(32, return_sequences=True))\nattention = SeqSelfAttention(attention_activation='relu')\ndropout_layer1 = Dropout(0.5)\nlstm_layer2 = Bidirectional(LSTM(16, return_sequences=True))\ndropout_layer2 = Dropout(0.5)\nlstm_layer3 = Bidirectional(LSTM(8, return_sequences=True))\ndropout_layer3 = Dropout(0.5)\npooling_layer = GlobalAveragePooling1D()\n\ninput1 = Input(shape=(145,))\ninput2 = Input(shape=(145,))\n\nencoded1 = lstm_layer1(batch_norm_layer(embedding_layer(input1)))\nencoded2 = lstm_layer1(batch_norm_layer(embedding_layer(input2)))\n\nencoded1 = dropout_layer1(attention(encoded1))\nencoded2 = dropout_layer1(attention(encoded2))\n\nmerged_layer = concatenate([encoded1, encoded2], axis=-1)\n\nmerged_layer = lstm_layer2(merged_layer)\nmerged_layer = dropout_layer2(merged_layer)\n\nmerged_layer = lstm_layer3(merged_layer)\nmerged_layer = dropout_layer3(merged_layer)\n\nmerged_layer = pooling_layer(merged_layer)\n\noutput_layer = Dense(1, activation='sigmoid')(merged_layer)\n\nmodel = Model(inputs=[input1, input2], outputs=output_layer)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n# Train the model\nhistory = model.fit([X_train, X_train], y_train, batch_size=32, epochs=20, validation_data=([X_test,X_test], y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy= model.evaluate([X_test,X_test], y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T14:30:35.225505Z","iopub.execute_input":"2023-04-23T14:30:35.225872Z","iopub.status.idle":"2023-04-23T14:44:02.464812Z","shell.execute_reply.started":"2023-04-23T14:30:35.225841Z","shell.execute_reply":"2023-04-23T14:44:02.463641Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.8054\nEpoch 1: val_loss improved from inf to 0.39124, saving model to best_model.h5\n2536/2536 [==============================] - 227s 84ms/step - loss: 0.4282 - accuracy: 0.8054 - val_loss: 0.3912 - val_accuracy: 0.8291\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.8521\nEpoch 2: val_loss did not improve from 0.39124\n2536/2536 [==============================] - 181s 72ms/step - loss: 0.3401 - accuracy: 0.8521 - val_loss: 0.4089 - val_accuracy: 0.8195\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.8869\nEpoch 3: val_loss did not improve from 0.39124\n2536/2536 [==============================] - 180s 71ms/step - loss: 0.2699 - accuracy: 0.8869 - val_loss: 0.4718 - val_accuracy: 0.8084\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9182\nEpoch 4: val_loss did not improve from 0.39124\n2536/2536 [==============================] - 172s 68ms/step - loss: 0.2003 - accuracy: 0.9182 - val_loss: 0.5368 - val_accuracy: 0.8015\nEpoch 4: early stopping\n634/634 [==============================] - 15s 24ms/step - loss: 0.3912 - accuracy: 0.8291\nTest accuracy: 0.8290939331054688\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\n\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Activation, Flatten, RepeatVector, Permute, Multiply, Lambda, Input, Reshape, GlobalAveragePooling1D, concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom keras import backend as K\nfrom keras_self_attention import SeqSelfAttention\n\n# Load the preprocessed data\ndata1 = pd.read_csv('/kaggle/input/unbalanced/trainu.csv')\ndata1.dropna(inplace=True)\nX = data1['comment_text_processed']\ny = data1['Target']\n\n# Tokenize the text data\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=145)\n\n# Oversample the minority class and undersample the majority class\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n\nclass_weights_dict = dict(enumerate(class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n\n# Build the Siamese BiLSTM model with attention layer\nembedding_layer = Embedding(input_dim=10000, output_dim=128)\nlstm_layer1 = Bidirectional(LSTM(64, return_sequences=True))\nattention = SeqSelfAttention(attention_activation='relu')\ndropout_layer1 = Dropout(0.5)\nlstm_layer2 = Bidirectional(LSTM(32, return_sequences=True))\ndropout_layer2 = Dropout(0.5)\nlstm_layer3 = Bidirectional(LSTM(16, return_sequences=True))\ndropout_layer3 = Dropout(0.5)\npooling_layer = GlobalAveragePooling1D()\n\ninput1 = Input(shape=(145,))\ninput2 = Input(shape=(145,))\n\nencoded1 = dropout_layer1(attention(lstm_layer1(embedding_layer(input1))))\nencoded2 = dropout_layer1(attention(lstm_layer1(embedding_layer(input2))))\n\nmerged_layer = concatenate([encoded1, encoded2], axis=-1)\n\nmerged_layer = lstm_layer2(merged_layer)\nmerged_layer = dropout_layer2(merged_layer)\n\nmerged_layer = lstm_layer3(merged_layer)\nmerged_layer = dropout_layer3(merged_layer)\n\nmerged_layer = pooling_layer(merged_layer)\n\noutput_layer = Dense(1, activation='sigmoid')(merged_layer)\n\nmodel = Model(inputs=[input1, input2], outputs=output_layer)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define callbacks for early stopping and saving the best model\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n\n# Train the model\nhistory = model.fit([X_train, X_train], y_train, batch_size=32, epochs=20, validation_data=([X_test,X_test], y_test), class_weight=class_weights_dict, callbacks=[early_stop, model_checkpoint])\n\n# Load the best model\nmodel.load_weights('best_model.h5')\n\n# Evaluate the model on the testing set\nloss, accuracy= model.evaluate([X_test,X_test], y_test, batch_size=32)\nprint('Test accuracy:', accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-26T01:27:12.349281Z","iopub.execute_input":"2023-04-26T01:27:12.349931Z","iopub.status.idle":"2023-04-26T01:43:29.651441Z","shell.execute_reply.started":"2023-04-26T01:27:12.349883Z","shell.execute_reply":"2023-04-26T01:43:29.649874Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  f\"The initializer {self.__class__.__name__} is unseeded \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.4375 - accuracy: 0.7932\nEpoch 1: val_loss improved from inf to 0.39574, saving model to best_model.h5\n2536/2536 [==============================] - 296s 108ms/step - loss: 0.4375 - accuracy: 0.7932 - val_loss: 0.3957 - val_accuracy: 0.8231\nEpoch 2/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.3556 - accuracy: 0.8460\nEpoch 2: val_loss did not improve from 0.39574\n2536/2536 [==============================] - 206s 81ms/step - loss: 0.3556 - accuracy: 0.8460 - val_loss: 0.4021 - val_accuracy: 0.8241\nEpoch 3/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2944 - accuracy: 0.8759\nEpoch 3: val_loss did not improve from 0.39574\n2536/2536 [==============================] - 204s 81ms/step - loss: 0.2944 - accuracy: 0.8759 - val_loss: 0.4466 - val_accuracy: 0.8170\nEpoch 4/20\n2536/2536 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9089\nEpoch 4: val_loss did not improve from 0.39574\n2536/2536 [==============================] - 204s 80ms/step - loss: 0.2226 - accuracy: 0.9089 - val_loss: 0.5184 - val_accuracy: 0.7997\nEpoch 4: early stopping\n634/634 [==============================] - 18s 28ms/step - loss: 0.3957 - accuracy: 0.8231\nTest accuracy: 0.8230799436569214\n","output_type":"stream"}]},{"cell_type":"code","source":"model.evaluate([x_test3,x_test3],y_test3)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:00:41.895201Z","iopub.execute_input":"2023-04-23T15:00:41.895910Z","iopub.status.idle":"2023-04-23T15:02:33.201289Z","shell.execute_reply.started":"2023-04-23T15:00:41.895870Z","shell.execute_reply":"2023-04-23T15:02:33.200133Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 111s 27ms/step - loss: 0.6082 - accuracy: 0.7306\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[0.6081710457801819, 0.7306397557258606]"},"metadata":{}}]},{"cell_type":"code","source":"model.evaluate([x_test3,x_test3],y_test3)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T01:47:03.007149Z","iopub.execute_input":"2023-04-26T01:47:03.007811Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 115s 27ms/step - loss: 0.6834 - accuracy: 0.6423\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred=model.predict([x_test3,x_test3])","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:18:21.616096Z","iopub.execute_input":"2023-04-23T15:18:21.616567Z","iopub.status.idle":"2023-04-23T15:20:43.759300Z","shell.execute_reply.started":"2023-04-23T15:18:21.616523Z","shell.execute_reply":"2023-04-23T15:20:43.758186Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"4187/4187 [==============================] - 99s 24ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"test4.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:23:43.214540Z","iopub.execute_input":"2023-04-23T15:23:43.215532Z","iopub.status.idle":"2023-04-23T15:23:43.235610Z","shell.execute_reply.started":"2023-04-23T15:23:43.215490Z","shell.execute_reply":"2023-04-23T15:23:43.234350Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                        comment_text  severe_toxicity  \\\n0  So between the 2 civil lawyers going for the j...                0   \n1  Hope they have bullet proof glass and bomb bar...                0   \n2  \"...They realize the inter-connectedness betwe...                0   \n3  I'm a Raider fan, but I agree with Finley.  Th...                0   \n4  I voted for Trump and it was not for any reaso...                0   \n\n   obscene  sexual_explicit  identity_attack  insult  threat  other  gender  \\\n0        0                0                0       0       0      1       0   \n1        0                0                0       0       0      1       0   \n2        0                0                0       0       0      1       0   \n3        0                0                0       0       0      1       0   \n4        0                0                0       0       0      1       0   \n\n   religion  race  disability  Target  \\\n0         0     0           0       0   \n1         0     0           0       0   \n2         0     0           0       0   \n3         0     0           0       0   \n4         0     0           0       0   \n\n                              processed_comment_text  \\\n0  ['civil', 'lawyer', 'going', 'job', 'one', 'st...   \n1  ['hope', 'bullet', 'proof', 'glass', 'bomb', '...   \n2  ['they', 'realize', 'interconnectedness', 'nat...   \n3  ['raider', 'fan', 'agree', 'finley', 'these', ...   \n4  ['voted', 'trump', 'reason', 'article', 'faceb...   \n\n                              comment_text_processed  \n0  civil lawyer going job one stellar reputation ...  \n1    hope bullet proof glass bomb barrier well armed  \n2  they realize interconnectedness nation world n...  \n3  raider fan agree finley these player sit anthe...  \n4  voted trump reason article facebook what mains...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>sexual_explicit</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>other</th>\n      <th>gender</th>\n      <th>religion</th>\n      <th>race</th>\n      <th>disability</th>\n      <th>Target</th>\n      <th>processed_comment_text</th>\n      <th>comment_text_processed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>So between the 2 civil lawyers going for the j...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['civil', 'lawyer', 'going', 'job', 'one', 'st...</td>\n      <td>civil lawyer going job one stellar reputation ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hope they have bullet proof glass and bomb bar...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['hope', 'bullet', 'proof', 'glass', 'bomb', '...</td>\n      <td>hope bullet proof glass bomb barrier well armed</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"...They realize the inter-connectedness betwe...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['they', 'realize', 'interconnectedness', 'nat...</td>\n      <td>they realize interconnectedness nation world n...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I'm a Raider fan, but I agree with Finley.  Th...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['raider', 'fan', 'agree', 'finley', 'these', ...</td>\n      <td>raider fan agree finley these player sit anthe...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I voted for Trump and it was not for any reaso...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['voted', 'trump', 'reason', 'article', 'faceb...</td>\n      <td>voted trump reason article facebook what mains...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Define the columns for which to make predictions\ncols_to_predict = [col for col in test4.columns if col not in ['comment_text', 'processed_comment_text', 'comment_text_processed','severe_toxicity','obscene','sexual_explicit','identity_attack','insult','threat','other','gender','religion','race','disability']]\n\n# Add predicted values to the test dataset\nfor i, col in enumerate(cols_to_predict):\n    test4[col + '_pred'] = y_pred[:, i]\n\n# Export the test dataset with predicted values to a CSV file\ntest4.to_csv('test_with_predictions_decimals_siamesebilstm_single.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:26:50.428121Z","iopub.execute_input":"2023-04-23T15:26:50.428926Z","iopub.status.idle":"2023-04-23T15:26:52.551322Z","shell.execute_reply.started":"2023-04-23T15:26:50.428884Z","shell.execute_reply":"2023-04-23T15:26:52.550182Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"pred=pd.read_csv('/kaggle/working/test_with_predictions_decimals_siamesebilstm_single.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:27:18.706012Z","iopub.execute_input":"2023-04-23T15:27:18.706425Z","iopub.status.idle":"2023-04-23T15:27:20.072779Z","shell.execute_reply.started":"2023-04-23T15:27:18.706387Z","shell.execute_reply":"2023-04-23T15:27:20.071728Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"pred.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:27:26.621960Z","iopub.execute_input":"2023-04-23T15:27:26.622703Z","iopub.status.idle":"2023-04-23T15:27:26.638559Z","shell.execute_reply.started":"2023-04-23T15:27:26.622660Z","shell.execute_reply":"2023-04-23T15:27:26.637418Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                                        comment_text  severe_toxicity  \\\n0  So between the 2 civil lawyers going for the j...                0   \n1  Hope they have bullet proof glass and bomb bar...                0   \n2  \"...They realize the inter-connectedness betwe...                0   \n3  I'm a Raider fan, but I agree with Finley.  Th...                0   \n4  I voted for Trump and it was not for any reaso...                0   \n\n   obscene  sexual_explicit  identity_attack  insult  threat  other  gender  \\\n0        0                0                0       0       0      1       0   \n1        0                0                0       0       0      1       0   \n2        0                0                0       0       0      1       0   \n3        0                0                0       0       0      1       0   \n4        0                0                0       0       0      1       0   \n\n   religion  race  disability  Target  \\\n0         0     0           0       0   \n1         0     0           0       0   \n2         0     0           0       0   \n3         0     0           0       0   \n4         0     0           0       0   \n\n                              processed_comment_text  \\\n0  ['civil', 'lawyer', 'going', 'job', 'one', 'st...   \n1  ['hope', 'bullet', 'proof', 'glass', 'bomb', '...   \n2  ['they', 'realize', 'interconnectedness', 'nat...   \n3  ['raider', 'fan', 'agree', 'finley', 'these', ...   \n4  ['voted', 'trump', 'reason', 'article', 'faceb...   \n\n                              comment_text_processed  Target_pred  \n0  civil lawyer going job one stellar reputation ...     0.228387  \n1    hope bullet proof glass bomb barrier well armed     0.500302  \n2  they realize interconnectedness nation world n...     0.046323  \n3  raider fan agree finley these player sit anthe...     0.157499  \n4  voted trump reason article facebook what mains...     0.302171  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>sexual_explicit</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>other</th>\n      <th>gender</th>\n      <th>religion</th>\n      <th>race</th>\n      <th>disability</th>\n      <th>Target</th>\n      <th>processed_comment_text</th>\n      <th>comment_text_processed</th>\n      <th>Target_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>So between the 2 civil lawyers going for the j...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['civil', 'lawyer', 'going', 'job', 'one', 'st...</td>\n      <td>civil lawyer going job one stellar reputation ...</td>\n      <td>0.228387</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hope they have bullet proof glass and bomb bar...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['hope', 'bullet', 'proof', 'glass', 'bomb', '...</td>\n      <td>hope bullet proof glass bomb barrier well armed</td>\n      <td>0.500302</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"...They realize the inter-connectedness betwe...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['they', 'realize', 'interconnectedness', 'nat...</td>\n      <td>they realize interconnectedness nation world n...</td>\n      <td>0.046323</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I'm a Raider fan, but I agree with Finley.  Th...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['raider', 'fan', 'agree', 'finley', 'these', ...</td>\n      <td>raider fan agree finley these player sit anthe...</td>\n      <td>0.157499</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I voted for Trump and it was not for any reaso...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['voted', 'trump', 'reason', 'article', 'faceb...</td>\n      <td>voted trump reason article facebook what mains...</td>\n      <td>0.302171</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Calculate F1 score\nfrom sklearn.metrics import f1_score\nf1 = f1_score(y_test3, y_pred.round(), average='macro')\nprint('F1 score:', f1)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:21:34.420677Z","iopub.execute_input":"2023-04-23T15:21:34.421444Z","iopub.status.idle":"2023-04-23T15:21:34.478520Z","shell.execute_reply.started":"2023-04-23T15:21:34.421399Z","shell.execute_reply":"2023-04-23T15:21:34.477174Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"F1 score: 0.5263960627286325\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nfilename = 'siamese_biLstm+attention_with_tuning1.pkl'\npickle.dump(model, open(filename, 'wb'))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T15:02:43.973670Z","iopub.execute_input":"2023-04-23T15:02:43.974096Z","iopub.status.idle":"2023-04-23T15:02:44.244771Z","shell.execute_reply.started":"2023-04-23T15:02:43.974059Z","shell.execute_reply":"2023-04-23T15:02:44.241066Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......bidirectional\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......bidirectional_1\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......bidirectional_2\n.........backward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........forward_layer\n............cell\n...............vars\n..................0\n..................1\n..................2\n............vars\n.........layer\n............cell\n...............vars\n............vars\n.........vars\n......concatenate\n.........vars\n......dense\n.........vars\n............0\n............1\n......dropout\n.........vars\n......dropout_1\n.........vars\n......dropout_2\n.........vars\n......embedding\n.........vars\n............0\n......global_average_pooling1d\n.........vars\n......input_layer\n.........vars\n......input_layer_1\n.........vars\n......seq_self_attention\n.........vars\n............0\n............1\n............2\n............3\n............4\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........17\n.........18\n.........19\n.........2\n.........20\n.........21\n.........22\n.........23\n.........24\n.........25\n.........26\n.........27\n.........28\n.........29\n.........3\n.........30\n.........31\n.........32\n.........33\n.........34\n.........35\n.........36\n.........37\n.........38\n.........39\n.........4\n.........40\n.........41\n.........42\n.........43\n.........44\n.........45\n.........46\n.........47\n.........48\n.........49\n.........5\n.........50\n.........51\n.........52\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2023-04-23 15:02:43         7204\nvariables.h5                                   2023-04-23 15:02:44     17754208\nmetadata.json                                  2023-04-23 15:02:43           64\n","output_type":"stream"}]}]}